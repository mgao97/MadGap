{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/users/Min/miniconda/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/users/Min/miniconda/lib/python3.11/site-packages/torch_geometric/typing.py:90: UserWarning: An issue occurred while importing 'torch-spline-conv'. Disabling its usage. Stacktrace: /users/Min/miniconda/lib/python3.11/site-packages/torch_spline_conv/_basis_cuda.so: undefined symbol: _ZNK3c107SymBool10guard_boolEPKcl\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.13.1+cu117\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import copy\n",
    "from tqdm import tqdm\n",
    "import networkx as nx\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from torch_geometric.utils import to_networkx\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch_geometric.utils import k_hop_subgraph\n",
    "from torch_geometric.datasets import Planetoid\n",
    "from torch_geometric.transforms import NormalizeFeatures\n",
    "from tqdm import tqdm\n",
    "import community as community_louvain\n",
    "from networkx.algorithms.community import greedy_modularity_communities, label_propagation_communities, louvain_communities\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE\n",
    "import optuna\n",
    "\n",
    "os.environ['TORCH'] = torch.__version__\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dataset: Cora():\n",
      "======================\n",
      "Number of graphs: 1\n",
      "Number of features: 1433\n",
      "Number of classes: 7\n",
      "\n",
      "================================================================================\n",
      "Number of nodes: 2708\n",
      "Number of edges: 10556\n",
      "Average node degree: 3.90\n",
      "Number of training nodes: 140\n",
      "Training node label rate: 0.05\n",
      "Has isolated nodes: False\n",
      "Has self-loops: False\n",
      "Is undirected: True\n"
     ]
    }
   ],
   "source": [
    "dataset = Planetoid(root='data/Planetoid', name='Cora', transform=NormalizeFeatures())\n",
    "print()\n",
    "print(f'Dataset: {dataset}:')\n",
    "print('======================')\n",
    "print(f'Number of graphs: {len(dataset)}')\n",
    "print(f'Number of features: {dataset.num_features}')\n",
    "print(f'Number of classes: {dataset.num_classes}')\n",
    "\n",
    "data = dataset[0]  # Get the first graph object.\n",
    "print()\n",
    "print('='*80)\n",
    "\n",
    "# Gather some statistics about the graph.\n",
    "print(f'Number of nodes: {data.num_nodes}')\n",
    "print(f'Number of edges: {data.num_edges}')\n",
    "print(f'Average node degree: {data.num_edges / data.num_nodes:.2f}')\n",
    "print(f'Number of training nodes: {data.train_mask.sum()}')\n",
    "print(f'Training node label rate: {int(data.train_mask.sum()) / data.num_nodes:.2f}')\n",
    "print(f'Has isolated nodes: {data.has_isolated_nodes()}')\n",
    "print(f'Has self-loops: {data.has_self_loops()}')\n",
    "print(f'Is undirected: {data.is_undirected()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]]),\n",
       " tensor(0.),\n",
       " tensor(1.))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.x, torch.min(data.x), torch.max(data.x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "feats = data.x.numpy()\n",
    "feats = feats.flatten()\n",
    "\n",
    "# 创建直方图\n",
    "plt.hist(feats, bins=50, color='blue', alpha=0.7)\n",
    "plt.title('Value Distribution of the Tensor')\n",
    "plt.xlabel('Value')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torch_geometric.data import Data\n",
    "\n",
    "# def community_detection(data, model_name, resolution):\n",
    "#     # Step 1: Convert Cora dataset to a NetworkX graph\n",
    "#     G = nx.Graph()\n",
    "#     for i in range(data.edge_index.shape[1]):\n",
    "#         source = data.edge_index[0, i].item()\n",
    "#         target = data.edge_index[1, i].item()\n",
    "#         G.add_edge(source, target)\n",
    "\n",
    "#     # Step 2: Apply Louvain community detection\n",
    "#     if model_name == 'louvain':\n",
    "#         partition = community_louvain.best_partition(G)\n",
    "#     elif model_name == 'greedy_modularity':\n",
    "#         communities = greedy_modularity_communities(G)\n",
    "#         partition = {}\n",
    "#         for idx, community in enumerate(communities):\n",
    "#             for node in community:\n",
    "#                 partition[node] = idx\n",
    "#     print(partition)\n",
    "#     # Step 3: Create a new graph based on community assignments\n",
    "#     # unique_partitions = np.unique(list(partition.values()))\n",
    "#     community_masks = torch.zeros((data.num_nodes, len(partition)), dtype=torch.float32)\n",
    "#     subgraphs = []\n",
    "\n",
    "#     for idx, community_id in enumerate(partition):\n",
    "#         # nodes_in_community = list(idx)\n",
    "#         nodes_in_community = [node for node, community in partition.items() if community == community_id]\n",
    "#         community_masks[nodes_in_community, idx] = 1\n",
    "\n",
    "#         # Extract subgraph for each community\n",
    "#         community_subgraph = G.subgraph(nodes_in_community)\n",
    "#         community_edges = torch.tensor(list(community_subgraph.edges)).t().contiguous()\n",
    "#         community_edge_index = torch.cat([community_edges, community_edges.flip(0)], dim=-1)\n",
    "#         community_data = Data(\n",
    "#             x=data.x[nodes_in_community],\n",
    "#             edge_index=community_edge_index,\n",
    "#             y=data.y[nodes_in_community],\n",
    "#             train_mask=data.train_mask[nodes_in_community],\n",
    "#             val_mask=data.val_mask[nodes_in_community],\n",
    "#             test_mask=data.test_mask[nodes_in_community]\n",
    "#         )\n",
    "#         subgraphs.append(community_data)\n",
    "\n",
    "#     return partition, community_masks, subgraphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.data import Data\n",
    "\n",
    "def community_detection(data, model_name, resolution):\n",
    "    # Step 1: Convert Cora dataset to a NetworkX graph\n",
    "    G = nx.Graph()\n",
    "    for i in range(data.edge_index.shape[1]):\n",
    "        source = data.edge_index[0, i].item()\n",
    "        target = data.edge_index[1, i].item()\n",
    "        G.add_edge(source, target)\n",
    "\n",
    "    # Step 2: Apply Louvain community detection\n",
    "    if model_name == 'louvain':\n",
    "        partition = louvain_communities(G, resolution)\n",
    "    elif model_name == 'greedy_modularity':\n",
    "        communities = greedy_modularity_communities(G)\n",
    "        partition = {}\n",
    "        for idx, community in enumerate(communities):\n",
    "            for node in community:\n",
    "                partition[node] = idx\n",
    "    partition1 = {}\n",
    "    for index, item in enumerate(partition):\n",
    "        for value in item:\n",
    "            partition1[value] = index\n",
    "    unique_partitions = np.unique(list(partition1.values()))\n",
    "    # Step 3: Create a new graph based on community assignments\n",
    "    # unique_partitions = np.unique(list(partition.values()))\n",
    "    community_masks = torch.zeros((data.num_nodes, len(unique_partitions)), dtype=torch.float32)\n",
    "    subgraphs = []\n",
    "\n",
    "    for idx, community_id in enumerate(unique_partitions):\n",
    "        nodes_in_community = [node for node, community in partition1.items() if community == community_id]\n",
    "        community_masks[nodes_in_community, idx] = 1\n",
    "\n",
    "        # Extract subgraph for each community\n",
    "        community_subgraph = G.subgraph(nodes_in_community)\n",
    "        community_edges = torch.tensor(list(community_subgraph.edges)).t().contiguous()\n",
    "        community_edge_index = torch.cat([community_edges, community_edges.flip(0)], dim=-1)\n",
    "        community_data = Data(\n",
    "            x=data.x[nodes_in_community],\n",
    "            edge_index=community_edge_index,\n",
    "            y=data.y[nodes_in_community],\n",
    "            train_mask=data.train_mask[nodes_in_community],\n",
    "            val_mask=data.val_mask[nodes_in_community],\n",
    "            test_mask=data.test_mask[nodes_in_community]\n",
    "        )\n",
    "        subgraphs.append(community_data)\n",
    "\n",
    "    return partition1, community_masks, subgraphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch_geometric.transforms as T\n",
    "\n",
    "# Load the Cora dataset\n",
    "cora_dataset = Planetoid(root='cora', name='Cora', transform=T.NormalizeFeatures())\n",
    "data = cora_dataset[0]\n",
    "\n",
    "# Apply community detection\n",
    "partition, community_masks, subgraphs = community_detection(data, model_name='louvain', resolution=0.5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{2544: 0,\n",
       " 3: 0,\n",
       " 208: 1,\n",
       " 7: 1,\n",
       " 2661: 2,\n",
       " 1318: 2,\n",
       " 2662: 2,\n",
       " 1001: 2,\n",
       " 12: 2,\n",
       " 0: 3,\n",
       " 2568: 3,\n",
       " 521: 3,\n",
       " 17: 3,\n",
       " 532: 3,\n",
       " 2582: 3,\n",
       " 24: 3,\n",
       " 1052: 3,\n",
       " 1055: 3,\n",
       " 1570: 3,\n",
       " 547: 3,\n",
       " 1574: 3,\n",
       " 553: 3,\n",
       " 1066: 3,\n",
       " 2606: 3,\n",
       " 1583: 3,\n",
       " 2611: 3,\n",
       " 54: 3,\n",
       " 570: 3,\n",
       " 571: 3,\n",
       " 1603: 3,\n",
       " 68: 3,\n",
       " 71: 3,\n",
       " 1100: 3,\n",
       " 598: 3,\n",
       " 1110: 3,\n",
       " 2650: 3,\n",
       " 2139: 3,\n",
       " 2140: 3,\n",
       " 2141: 3,\n",
       " 2142: 3,\n",
       " 96: 3,\n",
       " 1120: 3,\n",
       " 1636: 3,\n",
       " 1126: 3,\n",
       " 1127: 3,\n",
       " 618: 3,\n",
       " 1132: 3,\n",
       " 620: 3,\n",
       " 633: 3,\n",
       " 2170: 3,\n",
       " 2171: 3,\n",
       " 2691: 3,\n",
       " 649: 3,\n",
       " 1166: 3,\n",
       " 2191: 3,\n",
       " 143: 3,\n",
       " 1679: 3,\n",
       " 2706: 3,\n",
       " 2707: 3,\n",
       " 1172: 3,\n",
       " 149: 3,\n",
       " 1171: 3,\n",
       " 664: 3,\n",
       " 157: 3,\n",
       " 670: 3,\n",
       " 160: 3,\n",
       " 1701: 3,\n",
       " 165: 3,\n",
       " 2217: 3,\n",
       " 169: 3,\n",
       " 681: 3,\n",
       " 1708: 3,\n",
       " 176: 3,\n",
       " 690: 3,\n",
       " 179: 3,\n",
       " 696: 3,\n",
       " 185: 3,\n",
       " 1212: 3,\n",
       " 197: 3,\n",
       " 201: 3,\n",
       " 206: 3,\n",
       " 215: 3,\n",
       " 216: 3,\n",
       " 1239: 3,\n",
       " 1247: 3,\n",
       " 226: 3,\n",
       " 231: 3,\n",
       " 232: 3,\n",
       " 743: 3,\n",
       " 745: 3,\n",
       " 235: 3,\n",
       " 766: 3,\n",
       " 767: 3,\n",
       " 261: 3,\n",
       " 2313: 3,\n",
       " 2314: 3,\n",
       " 267: 3,\n",
       " 1295: 3,\n",
       " 784: 3,\n",
       " 1297: 3,\n",
       " 1299: 3,\n",
       " 1301: 3,\n",
       " 277: 3,\n",
       " 1820: 3,\n",
       " 1823: 3,\n",
       " 288: 3,\n",
       " 1315: 3,\n",
       " 1316: 3,\n",
       " 297: 3,\n",
       " 1323: 3,\n",
       " 304: 3,\n",
       " 311: 3,\n",
       " 314: 3,\n",
       " 1852: 3,\n",
       " 1853: 3,\n",
       " 1854: 3,\n",
       " 316: 3,\n",
       " 1855: 3,\n",
       " 1857: 3,\n",
       " 1858: 3,\n",
       " 1859: 3,\n",
       " 1860: 3,\n",
       " 1861: 3,\n",
       " 1347: 3,\n",
       " 1863: 3,\n",
       " 1864: 3,\n",
       " 1865: 3,\n",
       " 1866: 3,\n",
       " 2373: 3,\n",
       " 1862: 3,\n",
       " 1868: 3,\n",
       " 845: 3,\n",
       " 1870: 3,\n",
       " 1357: 3,\n",
       " 1872: 3,\n",
       " 1873: 3,\n",
       " 335: 3,\n",
       " 1875: 3,\n",
       " 1876: 3,\n",
       " 343: 3,\n",
       " 869: 3,\n",
       " 1893: 3,\n",
       " 873: 3,\n",
       " 874: 3,\n",
       " 2412: 3,\n",
       " 366: 3,\n",
       " 2430: 3,\n",
       " 387: 3,\n",
       " 899: 3,\n",
       " 391: 3,\n",
       " 909: 3,\n",
       " 920: 3,\n",
       " 1434: 3,\n",
       " 923: 3,\n",
       " 926: 3,\n",
       " 927: 3,\n",
       " 416: 3,\n",
       " 1440: 3,\n",
       " 1445: 3,\n",
       " 1446: 3,\n",
       " 938: 3,\n",
       " 1453: 3,\n",
       " 1456: 3,\n",
       " 1457: 3,\n",
       " 445: 3,\n",
       " 960: 3,\n",
       " 1473: 3,\n",
       " 1986: 3,\n",
       " 1987: 3,\n",
       " 1989: 3,\n",
       " 1990: 3,\n",
       " 1479: 3,\n",
       " 968: 3,\n",
       " 1994: 3,\n",
       " 1995: 3,\n",
       " 460: 3,\n",
       " 1997: 3,\n",
       " 2000: 3,\n",
       " 467: 3,\n",
       " 2004: 3,\n",
       " 2005: 3,\n",
       " 469: 3,\n",
       " 2007: 3,\n",
       " 2009: 3,\n",
       " 2532: 3,\n",
       " 2023: 3,\n",
       " 2025: 3,\n",
       " 2027: 3,\n",
       " 2028: 3,\n",
       " 1019: 3,\n",
       " 510: 3,\n",
       " 1023: 3,\n",
       " 99: 4,\n",
       " 122: 4,\n",
       " 2604: 4,\n",
       " 2454: 4,\n",
       " 2455: 4,\n",
       " 26: 4,\n",
       " 123: 4,\n",
       " 127: 4,\n",
       " 1594: 5,\n",
       " 31: 5,\n",
       " 523: 6,\n",
       " 782: 6,\n",
       " 783: 6,\n",
       " 271: 6,\n",
       " 1046: 6,\n",
       " 1051: 6,\n",
       " 2587: 6,\n",
       " 285: 6,\n",
       " 286: 6,\n",
       " 2335: 6,\n",
       " 543: 6,\n",
       " 33: 6,\n",
       " 2337: 6,\n",
       " 2336: 6,\n",
       " 1061: 6,\n",
       " 808: 6,\n",
       " 810: 6,\n",
       " 299: 6,\n",
       " 2348: 6,\n",
       " 813: 6,\n",
       " 814: 6,\n",
       " 1068: 6,\n",
       " 2615: 6,\n",
       " 2617: 6,\n",
       " 2362: 6,\n",
       " 1082: 6,\n",
       " 574: 6,\n",
       " 576: 6,\n",
       " 2119: 6,\n",
       " 2120: 6,\n",
       " 2121: 6,\n",
       " 2122: 6,\n",
       " 2123: 6,\n",
       " 2378: 6,\n",
       " 2379: 6,\n",
       " 2376: 6,\n",
       " 588: 6,\n",
       " 2637: 6,\n",
       " 2382: 6,\n",
       " 2383: 6,\n",
       " 589: 6,\n",
       " 1618: 6,\n",
       " 339: 6,\n",
       " 2640: 6,\n",
       " 2386: 6,\n",
       " 2387: 6,\n",
       " 91: 6,\n",
       " 1635: 6,\n",
       " 2406: 6,\n",
       " 1385: 6,\n",
       " 617: 6,\n",
       " 627: 6,\n",
       " 1143: 6,\n",
       " 121: 6,\n",
       " 330: 6,\n",
       " 893: 6,\n",
       " 638: 6,\n",
       " 1406: 6,\n",
       " 1158: 6,\n",
       " 332: 6,\n",
       " 2696: 6,\n",
       " 1162: 6,\n",
       " 911: 6,\n",
       " 1425: 6,\n",
       " 147: 6,\n",
       " 1428: 6,\n",
       " 1429: 6,\n",
       " 665: 6,\n",
       " 922: 6,\n",
       " 924: 6,\n",
       " 1436: 6,\n",
       " 166: 6,\n",
       " 429: 6,\n",
       " 1202: 6,\n",
       " 949: 6,\n",
       " 1976: 6,\n",
       " 442: 6,\n",
       " 698: 6,\n",
       " 705: 6,\n",
       " 2498: 6,\n",
       " 707: 6,\n",
       " 196: 6,\n",
       " 2380: 6,\n",
       " 714: 6,\n",
       " 2251: 6,\n",
       " 2252: 6,\n",
       " 1484: 6,\n",
       " 2001: 6,\n",
       " 2002: 6,\n",
       " 2259: 6,\n",
       " 980: 6,\n",
       " 2003: 6,\n",
       " 2262: 6,\n",
       " 1493: 6,\n",
       " 984: 6,\n",
       " 2520: 6,\n",
       " 2524: 6,\n",
       " 1510: 6,\n",
       " 1511: 6,\n",
       " 1519: 6,\n",
       " 1015: 6,\n",
       " 245: 6,\n",
       " 502: 6,\n",
       " 503: 6,\n",
       " 2040: 6,\n",
       " 1018: 6,\n",
       " 2043: 6,\n",
       " 2044: 6,\n",
       " 1534: 6,\n",
       " 763: 6,\n",
       " 2563: 7,\n",
       " 2564: 7,\n",
       " 1545: 7,\n",
       " 1559: 7,\n",
       " 1050: 7,\n",
       " 283: 7,\n",
       " 1569: 7,\n",
       " 1575: 7,\n",
       " 1320: 7,\n",
       " 1071: 7,\n",
       " 1081: 7,\n",
       " 1597: 7,\n",
       " 1342: 7,\n",
       " 320: 7,\n",
       " 837: 7,\n",
       " 1349: 7,\n",
       " 82: 7,\n",
       " 1109: 7,\n",
       " 1366: 7,\n",
       " 2647: 7,\n",
       " 2648: 7,\n",
       " 2646: 7,\n",
       " 856: 7,\n",
       " 2393: 7,\n",
       " 602: 7,\n",
       " 348: 7,\n",
       " 1634: 7,\n",
       " 1379: 7,\n",
       " 1380: 7,\n",
       " 1639: 7,\n",
       " 361: 7,\n",
       " 1643: 7,\n",
       " 1390: 7,\n",
       " 2670: 7,\n",
       " 1135: 7,\n",
       " 368: 7,\n",
       " 2673: 7,\n",
       " 1138: 7,\n",
       " 2672: 7,\n",
       " 2674: 7,\n",
       " 2675: 7,\n",
       " 2678: 7,\n",
       " 2679: 7,\n",
       " 2677: 7,\n",
       " 2682: 7,\n",
       " 378: 7,\n",
       " 2684: 7,\n",
       " 381: 7,\n",
       " 2681: 7,\n",
       " 2671: 7,\n",
       " 2683: 7,\n",
       " 1422: 7,\n",
       " 148: 7,\n",
       " 1686: 7,\n",
       " 1688: 7,\n",
       " 1437: 7,\n",
       " 2462: 7,\n",
       " 1442: 7,\n",
       " 2473: 7,\n",
       " 2474: 7,\n",
       " 1449: 7,\n",
       " 1194: 7,\n",
       " 1451: 7,\n",
       " 428: 7,\n",
       " 688: 7,\n",
       " 177: 7,\n",
       " 432: 7,\n",
       " 1461: 7,\n",
       " 1206: 7,\n",
       " 956: 7,\n",
       " 194: 7,\n",
       " 450: 7,\n",
       " 198: 7,\n",
       " 2248: 7,\n",
       " 2249: 7,\n",
       " 2250: 7,\n",
       " 1230: 7,\n",
       " 209: 7,\n",
       " 1491: 7,\n",
       " 2518: 7,\n",
       " 726: 7,\n",
       " 473: 7,\n",
       " 217: 7,\n",
       " 996: 7,\n",
       " 1255: 7,\n",
       " 1257: 7,\n",
       " 2538: 7,\n",
       " 492: 7,\n",
       " 1260: 7,\n",
       " 243: 7,\n",
       " 1532: 7,\n",
       " 1533: 7,\n",
       " 2624: 8,\n",
       " 44: 8,\n",
       " 2701: 8,\n",
       " 1582: 8,\n",
       " 522: 9,\n",
       " 511: 9,\n",
       " 1548: 9,\n",
       " 13: 9,\n",
       " 14: 9,\n",
       " 2578: 9,\n",
       " 531: 9,\n",
       " 2075: 9,\n",
       " 2076: 9,\n",
       " 2077: 9,\n",
       " 27: 9,\n",
       " 1057: 9,\n",
       " 549: 9,\n",
       " 38: 9,\n",
       " 1062: 9,\n",
       " 1576: 9,\n",
       " 1065: 9,\n",
       " 2091: 9,\n",
       " 45: 9,\n",
       " 2605: 9,\n",
       " 2097: 9,\n",
       " 2098: 9,\n",
       " 1590: 9,\n",
       " 1591: 9,\n",
       " 568: 9,\n",
       " 63: 9,\n",
       " 579: 9,\n",
       " 1096: 9,\n",
       " 78: 9,\n",
       " 1615: 9,\n",
       " 79: 9,\n",
       " 2130: 9,\n",
       " 1621: 9,\n",
       " 85: 9,\n",
       " 603: 9,\n",
       " 1116: 9,\n",
       " 606: 9,\n",
       " 1633: 9,\n",
       " 104: 9,\n",
       " 2667: 9,\n",
       " 2668: 9,\n",
       " 1134: 9,\n",
       " 1654: 9,\n",
       " 636: 9,\n",
       " 1665: 9,\n",
       " 1160: 9,\n",
       " 158: 9,\n",
       " 159: 9,\n",
       " 2210: 9,\n",
       " 678: 9,\n",
       " 1704: 9,\n",
       " 1192: 9,\n",
       " 171: 9,\n",
       " 691: 9,\n",
       " 180: 9,\n",
       " 2230: 9,\n",
       " 2231: 9,\n",
       " 2233: 9,\n",
       " 1217: 9,\n",
       " 1219: 9,\n",
       " 1223: 9,\n",
       " 715: 9,\n",
       " 716: 9,\n",
       " 2265: 9,\n",
       " 218: 9,\n",
       " 2268: 9,\n",
       " 733: 9,\n",
       " 224: 9,\n",
       " 230: 9,\n",
       " 1261: 9,\n",
       " 1265: 9,\n",
       " 2291: 9,\n",
       " 2292: 9,\n",
       " 759: 9,\n",
       " 2296: 9,\n",
       " 2301: 9,\n",
       " 1278: 9,\n",
       " 2303: 9,\n",
       " 2302: 9,\n",
       " 2305: 9,\n",
       " 2306: 9,\n",
       " 2308: 9,\n",
       " 775: 9,\n",
       " 781: 9,\n",
       " 1806: 9,\n",
       " 1807: 9,\n",
       " 1808: 9,\n",
       " 1809: 9,\n",
       " 1810: 9,\n",
       " 1811: 9,\n",
       " 1812: 9,\n",
       " 1294: 9,\n",
       " 1814: 9,\n",
       " 790: 9,\n",
       " 1816: 9,\n",
       " 1817: 9,\n",
       " 791: 9,\n",
       " 1813: 9,\n",
       " 794: 9,\n",
       " 1821: 9,\n",
       " 1822: 9,\n",
       " 1818: 9,\n",
       " 293: 9,\n",
       " 1322: 9,\n",
       " 1324: 9,\n",
       " 1329: 9,\n",
       " 1331: 9,\n",
       " 309: 9,\n",
       " 2360: 9,\n",
       " 313: 9,\n",
       " 2361: 9,\n",
       " 829: 9,\n",
       " 835: 9,\n",
       " 1348: 9,\n",
       " 862: 9,\n",
       " 351: 9,\n",
       " 863: 9,\n",
       " 1889: 9,\n",
       " 2402: 9,\n",
       " 2403: 9,\n",
       " 1890: 9,\n",
       " 1891: 9,\n",
       " 864: 9,\n",
       " 2409: 9,\n",
       " 377: 9,\n",
       " 383: 9,\n",
       " 1418: 9,\n",
       " 396: 9,\n",
       " 912: 9,\n",
       " 1424: 9,\n",
       " 401: 9,\n",
       " 430: 9,\n",
       " 2487: 9,\n",
       " 2488: 9,\n",
       " 962: 9,\n",
       " 1988: 9,\n",
       " 453: 9,\n",
       " 975: 9,\n",
       " 1489: 9,\n",
       " 983: 9,\n",
       " 2008: 9,\n",
       " 1497: 9,\n",
       " 482: 9,\n",
       " 994: 9,\n",
       " 486: 9,\n",
       " 1004: 9,\n",
       " 2541: 9,\n",
       " 2034: 9,\n",
       " 2035: 9,\n",
       " 2037: 9,\n",
       " 2550: 9,\n",
       " 2039: 9,\n",
       " 2552: 9,\n",
       " 2038: 9,\n",
       " 2042: 9,\n",
       " 1020: 9,\n",
       " 2559: 9,\n",
       " 66: 10,\n",
       " 2631: 10,\n",
       " 583: 11,\n",
       " 75: 11,\n",
       " 2222: 11,\n",
       " 2223: 11,\n",
       " 2224: 11,\n",
       " 2225: 11,\n",
       " 2226: 11,\n",
       " 84: 11,\n",
       " 284: 11,\n",
       " 898: 12,\n",
       " 1165: 12,\n",
       " 144: 12,\n",
       " 145: 12,\n",
       " 2192: 12,\n",
       " 213: 12,\n",
       " 1836: 12,\n",
       " 23: 12,\n",
       " 537: 12,\n",
       " 92: 12,\n",
       " 1504: 12,\n",
       " 2209: 12,\n",
       " 1698: 12,\n",
       " 1835: 12,\n",
       " 108: 12,\n",
       " 2157: 12,\n",
       " 2158: 12,\n",
       " 2159: 12,\n",
       " 2160: 12,\n",
       " 1328: 12,\n",
       " 2161: 12,\n",
       " 495: 12,\n",
       " 1327: 12,\n",
       " 1647: 12,\n",
       " 1593: 12,\n",
       " 2622: 12,\n",
       " 2048: 13,\n",
       " 1030: 13,\n",
       " 519: 13,\n",
       " 10: 13,\n",
       " 1551: 13,\n",
       " 1552: 13,\n",
       " 530: 13,\n",
       " 18: 13,\n",
       " 1045: 13,\n",
       " 1560: 13,\n",
       " 1561: 13,\n",
       " 1564: 13,\n",
       " 2078: 13,\n",
       " 542: 13,\n",
       " 2080: 13,\n",
       " 2079: 13,\n",
       " 2082: 13,\n",
       " 1571: 13,\n",
       " 2084: 13,\n",
       " 1572: 13,\n",
       " 2086: 13,\n",
       " 2598: 13,\n",
       " 2085: 13,\n",
       " 2087: 13,\n",
       " 2089: 13,\n",
       " 2090: 13,\n",
       " 2092: 13,\n",
       " 556: 13,\n",
       " 2094: 13,\n",
       " 2093: 13,\n",
       " 1584: 13,\n",
       " 1072: 13,\n",
       " 2096: 13,\n",
       " 563: 13,\n",
       " 2106: 13,\n",
       " 2107: 13,\n",
       " 1088: 13,\n",
       " 1089: 13,\n",
       " 581: 13,\n",
       " 1609: 13,\n",
       " 1622: 13,\n",
       " 1623: 13,\n",
       " 1624: 13,\n",
       " 86: 13,\n",
       " 2143: 13,\n",
       " 608: 13,\n",
       " 102: 13,\n",
       " 103: 13,\n",
       " 1640: 13,\n",
       " 1638: 13,\n",
       " 109: 13,\n",
       " 112: 13,\n",
       " 1651: 13,\n",
       " 1140: 13,\n",
       " 1142: 13,\n",
       " 1144: 13,\n",
       " 1656: 13,\n",
       " 1146: 13,\n",
       " 124: 13,\n",
       " 126: 13,\n",
       " 133: 13,\n",
       " 1670: 13,\n",
       " 134: 13,\n",
       " 136: 13,\n",
       " 138: 13,\n",
       " 139: 13,\n",
       " 655: 13,\n",
       " 656: 13,\n",
       " 1681: 13,\n",
       " 1682: 13,\n",
       " 660: 13,\n",
       " 153: 13,\n",
       " 1180: 13,\n",
       " 1699: 13,\n",
       " 1705: 13,\n",
       " 2218: 13,\n",
       " 1193: 13,\n",
       " 1196: 13,\n",
       " 1197: 13,\n",
       " 36: 13,\n",
       " 695: 13,\n",
       " 699: 13,\n",
       " 1733: 13,\n",
       " 2254: 13,\n",
       " 719: 13,\n",
       " 2256: 13,\n",
       " 1245: 13,\n",
       " 1248: 13,\n",
       " 1251: 13,\n",
       " 228: 13,\n",
       " 1253: 13,\n",
       " 741: 13,\n",
       " 1767: 13,\n",
       " 2081: 13,\n",
       " 234: 13,\n",
       " 1770: 13,\n",
       " 2591: 13,\n",
       " 1771: 13,\n",
       " 236: 13,\n",
       " 1772: 13,\n",
       " 1774: 13,\n",
       " 1775: 13,\n",
       " 1776: 13,\n",
       " 1773: 13,\n",
       " 1777: 13,\n",
       " 1778: 13,\n",
       " 1270: 13,\n",
       " 1779: 13,\n",
       " 1780: 13,\n",
       " 1781: 13,\n",
       " 1783: 13,\n",
       " 1784: 13,\n",
       " 1782: 13,\n",
       " 1786: 13,\n",
       " 1785: 13,\n",
       " 2300: 13,\n",
       " 1789: 13,\n",
       " 1787: 13,\n",
       " 1791: 13,\n",
       " 1788: 13,\n",
       " 1790: 13,\n",
       " 773: 13,\n",
       " 1798: 13,\n",
       " 1799: 13,\n",
       " 1800: 13,\n",
       " 1289: 13,\n",
       " 1802: 13,\n",
       " 1797: 13,\n",
       " 1804: 13,\n",
       " 1805: 13,\n",
       " 2088: 13,\n",
       " 2318: 13,\n",
       " 2319: 13,\n",
       " 2320: 13,\n",
       " 2322: 13,\n",
       " 2326: 13,\n",
       " 2327: 13,\n",
       " 1304: 13,\n",
       " 2328: 13,\n",
       " 2295: 13,\n",
       " 294: 13,\n",
       " 302: 13,\n",
       " 303: 13,\n",
       " 306: 13,\n",
       " 308: 13,\n",
       " 1335: 13,\n",
       " 1336: 13,\n",
       " 1337: 13,\n",
       " 317: 13,\n",
       " 318: 13,\n",
       " 831: 13,\n",
       " 1856: 13,\n",
       " 1346: 13,\n",
       " 322: 13,\n",
       " 836: 13,\n",
       " 329: 13,\n",
       " 1871: 13,\n",
       " 852: 13,\n",
       " 1878: 13,\n",
       " 1367: 13,\n",
       " 342: 13,\n",
       " 859: 13,\n",
       " 350: 13,\n",
       " 878: 13,\n",
       " 887: 13,\n",
       " 384: 13,\n",
       " 1411: 13,\n",
       " 910: 13,\n",
       " 406: 13,\n",
       " 407: 13,\n",
       " 409: 13,\n",
       " 417: 13,\n",
       " 1448: 13,\n",
       " 937: 13,\n",
       " 426: 13,\n",
       " 2478: 13,\n",
       " 943: 13,\n",
       " 945: 13,\n",
       " 1459: 13,\n",
       " 948: 13,\n",
       " 958: 13,\n",
       " 452: 13,\n",
       " 2505: 13,\n",
       " 459: 13,\n",
       " 1998: 13,\n",
       " 1490: 13,\n",
       " 476: 13,\n",
       " 1505: 13,\n",
       " 1506: 13,\n",
       " 484: 13,\n",
       " 487: 13,\n",
       " 1512: 13,\n",
       " 2026: 13,\n",
       " 2032: 13,\n",
       " 1009: 13,\n",
       " 2545: 13,\n",
       " 1011: 13,\n",
       " 1012: 13,\n",
       " 505: 13,\n",
       " 2045: 13,\n",
       " 2046: 13,\n",
       " 2047: 13,\n",
       " 106: 14,\n",
       " 2461: 14,\n",
       " 2537: 15,\n",
       " 259: 15,\n",
       " 117: 15,\n",
       " 1793: 16,\n",
       " 258: 16,\n",
       " 515: 16,\n",
       " 1794: 16,\n",
       " 1795: 16,\n",
       " 1796: 16,\n",
       " 275: 16,\n",
       " 789: 16,\n",
       " 22: 16,\n",
       " 29: 16,\n",
       " 798: 16,\n",
       " 805: 16,\n",
       " 2297: 16,\n",
       " 39: 16,\n",
       " 2298: 16,\n",
       " 43: 16,\n",
       " 2299: 16,\n",
       " 1585: 16,\n",
       " 828: 16,\n",
       " 1087: 16,\n",
       " 2623: 16,\n",
       " 1350: 16,\n",
       " 1094: 16,\n",
       " 582: 16,\n",
       " 584: 16,\n",
       " 1102: 16,\n",
       " 340: 16,\n",
       " 2645: 16,\n",
       " 89: 16,\n",
       " 1369: 16,\n",
       " 2399: 16,\n",
       " 2400: 16,\n",
       " 2401: 16,\n",
       " 1381: 16,\n",
       " 362: 16,\n",
       " 623: 16,\n",
       " 1906: 16,\n",
       " 884: 16,\n",
       " 1653: 16,\n",
       " 1141: 16,\n",
       " 375: 16,\n",
       " 1401: 16,\n",
       " 1153: 16,\n",
       " 2434: 16,\n",
       " 2435: 16,\n",
       " 1157: 16,\n",
       " 140: 16,\n",
       " 151: 16,\n",
       " 152: 16,\n",
       " 2463: 16,\n",
       " 2464: 16,\n",
       " 2465: 16,\n",
       " 1443: 16,\n",
       " 1702: 16,\n",
       " 1703: 16,\n",
       " 1964: 16,\n",
       " 1965: 16,\n",
       " 1966: 16,\n",
       " 1967: 16,\n",
       " 1969: 16,\n",
       " 1970: 16,\n",
       " 1971: 16,\n",
       " 946: 16,\n",
       " 1207: 16,\n",
       " 443: 16,\n",
       " 444: 16,\n",
       " 2236: 16,\n",
       " 2237: 16,\n",
       " 2238: 16,\n",
       " 2240: 16,\n",
       " 2496: 16,\n",
       " 706: 16,\n",
       " 963: 16,\n",
       " 2239: 16,\n",
       " 463: 16,\n",
       " 1234: 16,\n",
       " 1240: 16,\n",
       " 488: 16,\n",
       " 1264: 16,\n",
       " 2289: 16,\n",
       " 2290: 16,\n",
       " 2547: 16,\n",
       " 2548: 16,\n",
       " 2549: 16,\n",
       " 2294: 16,\n",
       " 1010: 16,\n",
       " 248: 16,\n",
       " 761: 16,\n",
       " 1530: 16,\n",
       " 1531: 16,\n",
       " 997: 17,\n",
       " 1991: 17,\n",
       " 1768: 17,\n",
       " 1837: 17,\n",
       " 2325: 17,\n",
       " 182: 17,\n",
       " 183: 17,\n",
       " 508: 17,\n",
       " 1056: 18,\n",
       " 2482: 18,\n",
       " 2437: 18,\n",
       " 2438: 18,\n",
       " 167: 18,\n",
       " 168: 18,\n",
       " 833: 19,\n",
       " 2595: 19,\n",
       " 1641: 19,\n",
       " 1130: 19,\n",
       " 207: 19,\n",
       " 178: 19,\n",
       " 2036: 19,\n",
       " 345: 19,\n",
       " 637: 19,\n",
       " 2494: 19,\n",
       " 2495: 19,\n",
       " 184: 20,\n",
       " 520: 20,\n",
       " 1208: 21,\n",
       " 187: 21,\n",
       " 1: 22,\n",
       " 2: 22,\n",
       " 1666: 22,\n",
       " 900: 22,\n",
       " 1031: 22,\n",
       " 2698: 22,\n",
       " 652: 22,\n",
       " 141: 22,\n",
       " 654: 22,\n",
       " 788: 22,\n",
       " 2204: 22,\n",
       " 2205: 22,\n",
       " 2206: 22,\n",
       " 1951: 22,\n",
       " 1952: 22,\n",
       " 1573: 22,\n",
       " 2471: 22,\n",
       " 811: 22,\n",
       " 1454: 22,\n",
       " 48: 22,\n",
       " 49: 22,\n",
       " 2493: 22,\n",
       " 2381: 22,\n",
       " 855: 22,\n",
       " 2521: 22,\n",
       " 2398: 22,\n",
       " 740: 22,\n",
       " 1002: 22,\n",
       " 622: 22,\n",
       " 2033: 22,\n",
       " 886: 22,\n",
       " 1400: 22,\n",
       " 2041: 22,\n",
       " 1662: 22,\n",
       " 639: 22,\n",
       " 200: 23,\n",
       " 2676: 23,\n",
       " 1439: 23,\n",
       " 821: 24,\n",
       " 222: 24,\n",
       " 225: 25,\n",
       " 2255: 25,\n",
       " 1538: 26,\n",
       " 1410: 26,\n",
       " 517: 26,\n",
       " 2185: 26,\n",
       " 11: 26,\n",
       " 1803: 26,\n",
       " 270: 26,\n",
       " 659: 26,\n",
       " 2453: 26,\n",
       " 279: 26,\n",
       " 668: 26,\n",
       " 1058: 26,\n",
       " 424: 26,\n",
       " 1195: 26,\n",
       " 1839: 26,\n",
       " 816: 26,\n",
       " 1842: 26,\n",
       " 1203: 26,\n",
       " 436: 26,\n",
       " 2099: 26,\n",
       " 1844: 26,\n",
       " 1974: 26,\n",
       " 952: 26,\n",
       " 1977: 26,\n",
       " 2105: 26,\n",
       " 1979: 26,\n",
       " 1980: 26,\n",
       " 1981: 26,\n",
       " 190: 26,\n",
       " 1215: 26,\n",
       " ...}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "partition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]]),\n",
       " torch.Size([2708, 104]))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "community_masks, community_masks.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Data(x=[2, 1433], edge_index=[2, 2], y=[2], train_mask=[2], val_mask=[2], test_mask=[2])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subgraphs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCN(nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels):\n",
    "        super().__init__()\n",
    "        torch.manual_seed(42)\n",
    "        self.conv1 = GCNConv(in_channels, hidden_channels)\n",
    "        self.conv2 = GCNConv(hidden_channels, out_channels)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = x.relu()\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        out = self.conv2(x, edge_index)\n",
    "        return x, out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GCN(\n",
       "  (conv1): GCNConv(1433, 512)\n",
       "  (conv2): GCNConv(512, 7)\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = GCN(data.num_features, 512, dataset.num_classes)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0036],\n",
       "         [0.0000, 0.0000, 0.0015,  ..., 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "         ...,\n",
       "         [0.0000, 0.0212, 0.0000,  ..., 0.0071, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000,  ..., 0.0237, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0070, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],\n",
       "        grad_fn=<MulBackward0>),\n",
       " torch.Size([2708, 512]))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x, out = model(data.x, data.edge_index)\n",
    "x, x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 0.0070, -0.0019,  0.0010,  ..., -0.0016, -0.0019,  0.0007],\n",
       "         [-0.0008, -0.0002,  0.0025,  ..., -0.0048, -0.0040,  0.0066],\n",
       "         [ 0.0084, -0.0072, -0.0010,  ...,  0.0033, -0.0011, -0.0005],\n",
       "         ...,\n",
       "         [-0.0040,  0.0138, -0.0039,  ..., -0.0004, -0.0018, -0.0028],\n",
       "         [ 0.0102, -0.0034, -0.0043,  ...,  0.0041, -0.0012, -0.0017],\n",
       "         [ 0.0065, -0.0052, -0.0023,  ...,  0.0018, -0.0004, -0.0034]],\n",
       "        grad_fn=<AddBackward0>),\n",
       " torch.Size([2708, 7]))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out, out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]]),\n",
       " tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " tensor([0., 0., 0.,  ..., 0., 0., 0.]))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "community_masks, community_masks[:,0], community_masks[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0036],\n",
       "         [0.0000, 0.0000, 0.0015,  ..., 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "         ...,\n",
       "         [0.0000, 0.0212, 0.0000,  ..., 0.0071, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000,  ..., 0.0237, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0070, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],\n",
       "        grad_fn=<MulBackward0>),\n",
       " tensor([0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 7.0476e-03, 0.0000e+00, 3.3942e-03,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 1.7982e-03, 1.2336e-03, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         7.2840e-03, 8.0726e-03, 0.0000e+00, 0.0000e+00, 0.0000e+00, 9.9954e-03,\n",
       "         0.0000e+00, 0.0000e+00, 3.1209e-03, 0.0000e+00, 1.9103e-02, 0.0000e+00,\n",
       "         7.6150e-03, 0.0000e+00, 1.0128e-03, 7.1582e-03, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 5.2362e-03, 0.0000e+00, 1.0165e-03, 0.0000e+00, 2.3844e-02,\n",
       "         0.0000e+00, 0.0000e+00, 2.9587e-03, 8.0599e-03, 0.0000e+00, 0.0000e+00,\n",
       "         5.6153e-03, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.3774e-02, 5.2230e-03,\n",
       "         1.0731e-02, 9.1521e-03, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         8.8686e-03, 9.9514e-03, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 6.7264e-03, 1.8922e-02, 0.0000e+00, 2.6884e-03, 8.8408e-03,\n",
       "         0.0000e+00, 0.0000e+00, 2.3415e-02, 0.0000e+00, 7.4092e-03, 0.0000e+00,\n",
       "         0.0000e+00, 2.5815e-03, 0.0000e+00, 9.8702e-03, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 1.3312e-02, 0.0000e+00, 0.0000e+00, 5.6786e-03, 5.8711e-03,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.3007e-03,\n",
       "         4.7055e-03, 1.8818e-02, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         7.1566e-03, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 1.2588e-02, 0.0000e+00, 0.0000e+00, 3.3409e-05, 1.6879e-02,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 5.6803e-03,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 5.4716e-03, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         7.5150e-04, 6.0713e-03, 0.0000e+00, 1.3047e-02, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 3.9250e-04, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.0582e-02,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         7.2161e-03, 0.0000e+00, 0.0000e+00, 1.4817e-02, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 1.4191e-02, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 2.7150e-02, 0.0000e+00, 8.8882e-03, 0.0000e+00, 0.0000e+00,\n",
       "         2.1710e-02, 1.4791e-02, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         5.9302e-03, 0.0000e+00, 0.0000e+00, 2.8751e-03, 0.0000e+00, 0.0000e+00,\n",
       "         7.5497e-03, 0.0000e+00, 0.0000e+00, 0.0000e+00, 2.9443e-03, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 2.0585e-02, 0.0000e+00,\n",
       "         0.0000e+00, 3.5420e-03, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         9.3465e-03, 6.2676e-03, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 9.9491e-03, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 5.1134e-03, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 1.7055e-03, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 1.5903e-02, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 1.4186e-03, 0.0000e+00, 3.3870e-03, 0.0000e+00, 0.0000e+00,\n",
       "         4.9507e-03, 5.8681e-03, 3.7710e-03, 2.4429e-03, 1.0763e-02, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 6.4023e-03, 5.6407e-03, 4.7768e-03,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         5.9338e-03, 0.0000e+00, 0.0000e+00, 2.6267e-03, 0.0000e+00, 0.0000e+00,\n",
       "         4.4165e-03, 0.0000e+00, 3.4277e-03, 0.0000e+00, 0.0000e+00, 6.5567e-03,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 2.1821e-03, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 5.9497e-03, 0.0000e+00,\n",
       "         3.3117e-03, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 6.6850e-03, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         1.4072e-02, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 3.8768e-03, 0.0000e+00, 0.0000e+00,\n",
       "         5.0225e-03, 1.6115e-02, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         4.4419e-03, 0.0000e+00, 9.0944e-03, 0.0000e+00, 0.0000e+00, 3.7450e-03,\n",
       "         0.0000e+00, 6.8107e-03, 0.0000e+00, 0.0000e+00, 4.1117e-03, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         5.1284e-03, 2.1370e-03, 0.0000e+00, 0.0000e+00, 1.7464e-02, 0.0000e+00,\n",
       "         3.3156e-03, 5.1254e-03, 0.0000e+00, 2.0415e-02, 3.9987e-03, 1.2806e-02,\n",
       "         0.0000e+00, 7.3389e-03, 0.0000e+00, 6.7756e-03, 3.8063e-03, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 6.7345e-03,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 8.3159e-03, 2.0720e-02,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 6.4934e-03, 0.0000e+00, 2.1248e-02,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 2.5066e-03, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 2.3502e-03, 4.8972e-03,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 8.1808e-04, 7.8016e-03, 0.0000e+00,\n",
       "         0.0000e+00, 1.3917e-03, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         7.6626e-03, 8.5462e-03, 0.0000e+00, 1.8260e-02, 0.0000e+00, 1.1152e-02,\n",
       "         0.0000e+00, 0.0000e+00, 9.2682e-03, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         8.9944e-03, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 7.2637e-03,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 7.4994e-04,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.2756e-02, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.1976e-03, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         1.3243e-02, 3.0673e-03, 1.6560e-03, 7.7313e-03, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 3.5909e-03], grad_fn=<SliceBackward0>))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x, x[0,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([   3, 2544]),\n",
       " tensor([  7, 208]),\n",
       " tensor([  12, 1001, 1318, 2661, 2662]),\n",
       " tensor([   0,   17,   24,   54,   68,   71,   96,  143,  149,  157,  160,  165,\n",
       "          169,  176,  179,  185,  197,  201,  206,  215,  216,  226,  231,  232,\n",
       "          235,  261,  267,  277,  288,  297,  304,  311,  314,  316,  335,  343,\n",
       "          366,  387,  391,  416,  445,  460,  467,  469,  510,  521,  532,  547,\n",
       "          553,  570,  571,  598,  618,  620,  633,  649,  664,  670,  681,  690,\n",
       "          696,  743,  745,  766,  767,  784,  845,  869,  873,  874,  899,  909,\n",
       "          920,  923,  926,  927,  938,  960,  968, 1019, 1023, 1052, 1055, 1066,\n",
       "         1100, 1110, 1120, 1126, 1127, 1132, 1166, 1171, 1172, 1212, 1239, 1247,\n",
       "         1295, 1297, 1299, 1301, 1315, 1316, 1323, 1347, 1357, 1434, 1440, 1445,\n",
       "         1446, 1453, 1456, 1457, 1473, 1479, 1570, 1574, 1583, 1603, 1636, 1679,\n",
       "         1701, 1708, 1820, 1823, 1852, 1853, 1854, 1855, 1857, 1858, 1859, 1860,\n",
       "         1861, 1862, 1863, 1864, 1865, 1866, 1868, 1870, 1872, 1873, 1875, 1876,\n",
       "         1893, 1986, 1987, 1989, 1990, 1994, 1995, 1997, 2000, 2004, 2005, 2007,\n",
       "         2009, 2023, 2025, 2027, 2028, 2139, 2140, 2141, 2142, 2170, 2171, 2191,\n",
       "         2217, 2313, 2314, 2373, 2412, 2430, 2532, 2568, 2582, 2606, 2611, 2650,\n",
       "         2691, 2706, 2707]),\n",
       " tensor([  26,   99,  122,  123,  127, 2454, 2455, 2604]),\n",
       " tensor([  31, 1594]),\n",
       " tensor([  33,   91,  121,  147,  166,  196,  245,  271,  285,  286,  299,  330,\n",
       "          332,  339,  429,  442,  502,  503,  523,  543,  574,  576,  588,  589,\n",
       "          617,  627,  638,  665,  698,  705,  707,  714,  763,  782,  783,  808,\n",
       "          810,  813,  814,  893,  911,  922,  924,  949,  980,  984, 1015, 1018,\n",
       "         1046, 1051, 1061, 1068, 1082, 1143, 1158, 1162, 1202, 1385, 1406, 1425,\n",
       "         1428, 1429, 1436, 1484, 1493, 1510, 1511, 1519, 1534, 1618, 1635, 1976,\n",
       "         2001, 2002, 2003, 2040, 2043, 2044, 2119, 2120, 2121, 2122, 2123, 2251,\n",
       "         2252, 2259, 2262, 2335, 2336, 2337, 2348, 2362, 2376, 2378, 2379, 2380,\n",
       "         2382, 2383, 2386, 2387, 2406, 2498, 2520, 2524, 2587, 2615, 2617, 2637,\n",
       "         2640, 2696]),\n",
       " tensor([  82,  148,  177,  194,  198,  209,  217,  243,  283,  320,  348,  361,\n",
       "          368,  378,  381,  428,  432,  450,  473,  492,  602,  688,  726,  837,\n",
       "          856,  956,  996, 1050, 1071, 1081, 1109, 1135, 1138, 1194, 1206, 1230,\n",
       "         1255, 1257, 1260, 1320, 1342, 1349, 1366, 1379, 1380, 1390, 1422, 1437,\n",
       "         1442, 1449, 1451, 1461, 1491, 1532, 1533, 1545, 1559, 1569, 1575, 1597,\n",
       "         1634, 1639, 1643, 1686, 1688, 2248, 2249, 2250, 2393, 2462, 2473, 2474,\n",
       "         2518, 2538, 2563, 2564, 2646, 2647, 2648, 2670, 2671, 2672, 2673, 2674,\n",
       "         2675, 2677, 2678, 2679, 2681, 2682, 2683, 2684]),\n",
       " tensor([  44, 1582, 2624, 2701]),\n",
       " tensor([  13,   14,   27,   38,   45,   63,   78,   79,   85,  104,  158,  159,\n",
       "          171,  180,  218,  224,  230,  293,  309,  313,  351,  377,  383,  396,\n",
       "          401,  430,  453,  482,  486,  511,  522,  531,  549,  568,  579,  603,\n",
       "          606,  636,  678,  691,  715,  716,  733,  759,  775,  781,  790,  791,\n",
       "          794,  829,  835,  862,  863,  864,  912,  962,  975,  983,  994, 1004,\n",
       "         1020, 1057, 1062, 1065, 1096, 1116, 1134, 1160, 1192, 1217, 1219, 1223,\n",
       "         1261, 1265, 1278, 1294, 1322, 1324, 1329, 1331, 1348, 1418, 1424, 1489,\n",
       "         1497, 1548, 1576, 1590, 1591, 1615, 1621, 1633, 1654, 1665, 1704, 1806,\n",
       "         1807, 1808, 1809, 1810, 1811, 1812, 1813, 1814, 1816, 1817, 1818, 1821,\n",
       "         1822, 1889, 1890, 1891, 1988, 2008, 2034, 2035, 2037, 2038, 2039, 2042,\n",
       "         2075, 2076, 2077, 2091, 2097, 2098, 2130, 2210, 2230, 2231, 2233, 2265,\n",
       "         2268, 2291, 2292, 2296, 2301, 2302, 2303, 2305, 2306, 2308, 2360, 2361,\n",
       "         2402, 2403, 2409, 2487, 2488, 2541, 2550, 2552, 2559, 2578, 2605, 2667,\n",
       "         2668]),\n",
       " tensor([  66, 2631]),\n",
       " tensor([  75,   84,  284,  583, 2222, 2223, 2224, 2225, 2226]),\n",
       " tensor([  23,   92,  108,  144,  145,  213,  495,  537,  898, 1165, 1327, 1328,\n",
       "         1504, 1593, 1647, 1698, 1835, 1836, 2157, 2158, 2159, 2160, 2161, 2192,\n",
       "         2209, 2622]),\n",
       " tensor([  10,   18,   36,   86,  102,  103,  109,  112,  124,  126,  133,  134,\n",
       "          136,  138,  139,  153,  228,  234,  236,  294,  302,  303,  306,  308,\n",
       "          317,  318,  322,  329,  342,  350,  384,  406,  407,  409,  417,  426,\n",
       "          452,  459,  476,  484,  487,  505,  519,  530,  542,  556,  563,  581,\n",
       "          608,  655,  656,  660,  695,  699,  719,  741,  773,  831,  836,  852,\n",
       "          859,  878,  887,  910,  937,  943,  945,  948,  958, 1009, 1011, 1012,\n",
       "         1030, 1045, 1072, 1088, 1089, 1140, 1142, 1144, 1146, 1180, 1193, 1196,\n",
       "         1197, 1245, 1248, 1251, 1253, 1270, 1289, 1304, 1335, 1336, 1337, 1346,\n",
       "         1367, 1411, 1448, 1459, 1490, 1505, 1506, 1512, 1551, 1552, 1560, 1561,\n",
       "         1564, 1571, 1572, 1584, 1609, 1622, 1623, 1624, 1638, 1640, 1651, 1656,\n",
       "         1670, 1681, 1682, 1699, 1705, 1733, 1767, 1770, 1771, 1772, 1773, 1774,\n",
       "         1775, 1776, 1777, 1778, 1779, 1780, 1781, 1782, 1783, 1784, 1785, 1786,\n",
       "         1787, 1788, 1789, 1790, 1791, 1797, 1798, 1799, 1800, 1802, 1804, 1805,\n",
       "         1856, 1871, 1878, 1998, 2026, 2032, 2045, 2046, 2047, 2048, 2078, 2079,\n",
       "         2080, 2081, 2082, 2084, 2085, 2086, 2087, 2088, 2089, 2090, 2092, 2093,\n",
       "         2094, 2096, 2106, 2107, 2143, 2218, 2254, 2256, 2295, 2300, 2318, 2319,\n",
       "         2320, 2322, 2326, 2327, 2328, 2478, 2505, 2545, 2591, 2598]),\n",
       " tensor([ 106, 2461]),\n",
       " tensor([ 117,  259, 2537]),\n",
       " tensor([  22,   29,   39,   43,   89,  140,  151,  152,  248,  258,  275,  340,\n",
       "          362,  375,  443,  444,  463,  488,  515,  582,  584,  623,  706,  761,\n",
       "          789,  798,  805,  828,  884,  946,  963, 1010, 1087, 1094, 1102, 1141,\n",
       "         1153, 1157, 1207, 1234, 1240, 1264, 1350, 1369, 1381, 1401, 1443, 1530,\n",
       "         1531, 1585, 1653, 1702, 1703, 1793, 1794, 1795, 1796, 1906, 1964, 1965,\n",
       "         1966, 1967, 1969, 1970, 1971, 2236, 2237, 2238, 2239, 2240, 2289, 2290,\n",
       "         2294, 2297, 2298, 2299, 2399, 2400, 2401, 2434, 2435, 2463, 2464, 2465,\n",
       "         2496, 2547, 2548, 2549, 2623, 2645]),\n",
       " tensor([ 182,  183,  508,  997, 1768, 1837, 1991, 2325]),\n",
       " tensor([ 167,  168, 1056, 2437, 2438, 2482]),\n",
       " tensor([ 178,  207,  345,  637,  833, 1130, 1641, 2036, 2494, 2495, 2595]),\n",
       " tensor([184, 520]),\n",
       " tensor([ 187, 1208]),\n",
       " tensor([   1,    2,   48,   49,  141,  622,  639,  652,  654,  740,  788,  811,\n",
       "          855,  886,  900, 1002, 1031, 1400, 1454, 1573, 1662, 1666, 1951, 1952,\n",
       "         2033, 2041, 2204, 2205, 2206, 2381, 2398, 2471, 2493, 2521, 2698]),\n",
       " tensor([ 200, 1439, 2676]),\n",
       " tensor([222, 821]),\n",
       " tensor([ 225, 2255]),\n",
       " tensor([  11,   77,  120,  190,  242,  270,  279,  354,  424,  436,  483,  490,\n",
       "          491,  517,  578,  659,  668,  816,  838,  868,  952,  988, 1058, 1105,\n",
       "         1121, 1131, 1133, 1195, 1203, 1215, 1341, 1410, 1500, 1526, 1538, 1655,\n",
       "         1803, 1839, 1842, 1844, 1879, 1974, 1977, 1979, 1980, 1981, 2099, 2105,\n",
       "         2135, 2136, 2164, 2185, 2276, 2280, 2281, 2282, 2384, 2405, 2425, 2426,\n",
       "         2453]),\n",
       " tensor([ 247, 2583]),\n",
       " tensor([ 250, 2429]),\n",
       " tensor([   5,    9,   16,   28,   34,   46,   51,   53,   58,   59,   64,   72,\n",
       "           73,   83,  105,  110,  111,  131,  154,  170,  173,  174,  188,  189,\n",
       "          202,  223,  227,  244,  246,  263,  268,  289,  290,  296,  301,  312,\n",
       "          326,  328,  333,  337,  341,  346,  353,  357,  358,  363,  364,  367,\n",
       "          372,  403,  421,  425,  427,  439,  449,  457,  464,  466,  480,  481,\n",
       "          489,  494,  501,  513,  524,  526,  533,  534,  535,  536,  552,  557,\n",
       "          558,  564,  565,  567,  580,  609,  613,  615,  616,  626,  628,  629,\n",
       "          631,  632,  650,  657,  683,  686,  687,  689,  708,  710,  723,  732,\n",
       "          744,  748,  752,  757,  758,  760,  762,  764,  768,  796,  797,  800,\n",
       "          806,  819,  820,  823,  824,  825,  834,  848,  853,  860,  867,  871,\n",
       "          876,  881,  882,  883,  889,  902,  915,  919,  929,  931,  933,  936,\n",
       "          951,  957,  970,  976,  977,  978,  985,  987,  998,  999, 1003, 1007,\n",
       "         1033, 1035, 1037, 1038, 1040, 1041, 1067, 1103, 1114, 1123, 1136, 1145,\n",
       "         1148, 1149, 1150, 1154, 1155, 1161, 1164, 1169, 1173, 1175, 1189, 1199,\n",
       "         1201, 1209, 1214, 1216, 1229, 1238, 1243, 1249, 1250, 1252, 1254, 1262,\n",
       "         1280, 1281, 1284, 1287, 1292, 1305, 1306, 1308, 1317, 1339, 1345, 1358,\n",
       "         1361, 1373, 1374, 1383, 1384, 1389, 1392, 1398, 1426, 1427, 1430, 1431,\n",
       "         1433, 1466, 1471, 1480, 1486, 1492, 1499, 1509, 1513, 1516, 1520, 1528,\n",
       "         1546, 1547, 1550, 1557, 1562, 1565, 1568, 1586, 1589, 1596, 1599, 1601,\n",
       "         1604, 1606, 1610, 1612, 1617, 1625, 1627, 1629, 1632, 1642, 1646, 1659,\n",
       "         1678, 1685, 1687, 1694, 1706, 1709, 1710, 1711, 1712, 1714, 1715, 1716,\n",
       "         1717, 1718, 1719, 1720, 1721, 1722, 1723, 1724, 1725, 1726, 1727, 1729,\n",
       "         1730, 1731, 1734, 1737, 1738, 1739, 1740, 1742, 1744, 1745, 1746, 1747,\n",
       "         1748, 1749, 1750, 1751, 1752, 1753, 1754, 1755, 1756, 1757, 1758, 1760,\n",
       "         1762, 1764, 1765, 1766, 1830, 1831, 1832, 1833, 1834, 1886, 1887, 1888,\n",
       "         2083, 2167, 2168, 2169, 2212, 2213, 2214, 2215, 2216, 2220, 2221, 2279,\n",
       "         2307, 2334, 2359, 2365, 2366, 2372, 2377, 2389, 2390, 2391, 2392, 2413,\n",
       "         2414, 2415, 2421, 2439, 2440, 2441, 2442, 2443, 2444, 2445, 2446, 2447,\n",
       "         2448, 2451, 2452, 2459, 2460, 2466, 2476, 2492, 2503, 2507, 2514, 2515,\n",
       "         2522, 2523, 2525, 2530, 2533, 2546, 2554, 2556, 2557, 2558, 2560, 2561,\n",
       "         2581, 2584, 2585, 2596, 2597, 2607, 2612, 2614, 2616, 2627, 2641, 2642,\n",
       "         2643, 2651, 2652, 2654, 2655, 2656, 2657, 2658, 2685, 2689]),\n",
       " tensor([  21,   25,   30,   55,   57,   61,   65,   76,   87,   88,   93,   98,\n",
       "          130,  146,  161,  162,  164,  191,  204,  210,  211,  214,  239,  249,\n",
       "          272,  300,  305,  323,  324,  325,  334,  349,  356,  382,  405,  415,\n",
       "          419,  435,  437,  438,  454,  461,  465,  471,  478,  479,  493,  497,\n",
       "          498,  514,  516,  529,  550,  566,  569,  572,  593,  619,  621,  634,\n",
       "          645,  651,  673,  674,  693,  697,  717,  718,  724,  737,  738,  753,\n",
       "          755,  769,  771,  772,  787,  815,  818,  827,  841,  842,  851,  885,\n",
       "          890,  891,  896,  897,  908,  942,  950,  966,  995, 1013, 1021, 1039,\n",
       "         1043, 1069, 1080, 1152, 1156, 1174, 1178, 1220, 1227, 1244, 1259, 1266,\n",
       "         1268, 1269, 1274, 1275, 1277, 1288, 1293, 1309, 1314, 1332, 1338, 1344,\n",
       "         1394, 1403, 1417, 1419, 1435, 1447, 1452, 1468, 1474, 1487, 1494, 1495,\n",
       "         1521, 1529, 1566, 1614, 1626, 1630, 1644, 1658, 1661, 1667, 1671, 1674,\n",
       "         1677, 1696, 1713, 1732, 1741, 1759, 1840, 1843, 1845, 1847, 1848, 1851,\n",
       "         1880, 1882, 1895, 1898, 1900, 1905, 1907, 1908, 1909, 1912, 1923, 1924,\n",
       "         1927, 1928, 1929, 1930, 1968, 1978, 1982, 1983, 1984, 1985, 1999, 2010,\n",
       "         2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020, 2021, 2022,\n",
       "         2024, 2059, 2071, 2102, 2103, 2104, 2108, 2137, 2152, 2156, 2162, 2178,\n",
       "         2188, 2190, 2193, 2194, 2260, 2261, 2304, 2309, 2310, 2315, 2317, 2342,\n",
       "         2343, 2344, 2350, 2385, 2404, 2416, 2417, 2418, 2419, 2467, 2468, 2475,\n",
       "         2512, 2593, 2608]),\n",
       " tensor([ 287, 2705]),\n",
       " tensor([  90,  155,  156,  220,  221,  376,  672,  817,  913, 1689, 1736, 1763,\n",
       "         2456]),\n",
       " tensor([ 292, 1036, 2562]),\n",
       " tensor([307, 991]),\n",
       " tensor([ 212,  273,  374,  413,  455,  509,  607,  658,  785,  846, 1084, 1101,\n",
       "         1213, 1242, 1290, 1664, 1684, 2146, 2147, 2148, 2149, 2258, 2486, 2513]),\n",
       " tensor([ 369,  385, 2483, 2484]),\n",
       " tensor([ 380,  477,  930, 2569]),\n",
       " tensor([ 390, 1108]),\n",
       " tensor([ 404, 1170, 1476]),\n",
       " tensor([422, 545]),\n",
       " tensor([ 431, 2694, 2695]),\n",
       " tensor([ 462, 1048]),\n",
       " tensor([ 474, 1181]),\n",
       " tensor([ 538, 1286]),\n",
       " tensor([ 560,  585,  774, 2526]),\n",
       " tensor([ 587, 1032]),\n",
       " tensor([ 172,  192,  240,  256,  260,  278,  291,  365,  433,  512,  591,  614,\n",
       "          642,  756,  888, 1017, 1078, 1185, 1423, 1460, 1469, 1472, 1475, 1498,\n",
       "         1514, 1556, 1692, 1992, 2187, 2345, 2346, 2349, 2358, 2469, 2470, 2472,\n",
       "         2589, 2590]),\n",
       " tensor([ 592, 2669]),\n",
       " tensor([ 611, 2690]),\n",
       " tensor([ 625, 1024]),\n",
       " tensor([ 635, 1378, 1544, 2058, 2150]),\n",
       " tensor([ 641, 2704]),\n",
       " tensor([ 359,  389,  562,  684,  704,  742,  754,  905,  969, 1077, 1282, 1283,\n",
       "         1412, 1483, 1620, 1735, 1743, 2113, 2208, 2324, 2450]),\n",
       " tensor([ 653, 1231]),\n",
       " tensor([662, 932]),\n",
       " tensor([  94,  195,  229,  386,  675,  770,  934, 1053, 1365, 1405, 1508, 1649,\n",
       "         1846, 1867, 1894, 2114, 2115, 2117, 2118, 2211, 2263, 2355, 2356, 2357,\n",
       "         2424, 2490, 2509, 2519, 2588]),\n",
       " tensor([ 677,  954, 1112]),\n",
       " tensor([   6,   52,   62,   74,   80,   81,   95,  100,  142,  257,  276,  315,\n",
       "          347,  373,  399,  408,  423,  434,  456,  475,  485,  525,  527,  544,\n",
       "          555,  667,  679,  680,  731,  734,  736,  751,  839,  847,  858,  861,\n",
       "          865,  901,  964,  965, 1006, 1008, 1025, 1042, 1047, 1117, 1118, 1125,\n",
       "         1139, 1198, 1204, 1279, 1302, 1330, 1376, 1388, 1391, 1395, 1397, 1416,\n",
       "         1463, 1467, 1481, 1515, 1517, 1524, 1540, 1580, 1588, 1602, 1628, 1631,\n",
       "         1669, 1680, 1693, 1695, 1769, 1801, 1838, 1841, 1892, 1920, 1921, 1922,\n",
       "         1925, 1926, 2049, 2051, 2052, 2053, 2054, 2055, 2056, 2057, 2068, 2072,\n",
       "         2073, 2074, 2131, 2133, 2172, 2180, 2181, 2182, 2183, 2186, 2189, 2196,\n",
       "         2197, 2198, 2199, 2200, 2201, 2202, 2203, 2219, 2232, 2234, 2266, 2267,\n",
       "         2277, 2311, 2321, 2330, 2332, 2333, 2347, 2364, 2408, 2422, 2499, 2502,\n",
       "         2511, 2575, 2576, 2620, 2653]),\n",
       " tensor([ 692, 2629]),\n",
       " tensor([ 700, 1691]),\n",
       " tensor([ 713, 1044]),\n",
       " tensor([ 721, 1034]),\n",
       " tensor([  50,   67,   70,   97,  113,  114,  129,  199,  203,  205,  241,  265,\n",
       "          282,  344,  393,  410,  420,  441,  470,  540,  590,  610,  612,  648,\n",
       "          661,  676,  701,  735,  739,  747,  750,  802,  804,  807,  826,  840,\n",
       "          854,  857, 1028, 1060, 1097, 1115, 1128, 1179, 1226, 1237, 1246, 1307,\n",
       "         1353, 1441, 1444, 1455, 1537, 1543, 1700, 1707, 1869, 1881, 1883, 1884,\n",
       "         1885, 1954, 1955, 1956, 1958, 2060, 2061, 2062, 2184, 2235, 2271, 2272,\n",
       "         2273, 2275, 2287, 2288, 2323, 2481, 2506, 2527, 2528, 2536, 2628, 2644]),\n",
       " tensor([  32,   37,   41,   42,   56,   60,   69,  115,  116,  175,  181,  252,\n",
       "          262,  266,  274,  280,  331,  355,  412,  414,  447,  451,  496,  499,\n",
       "          504,  506,  518,  528,  539,  551,  586,  594,  596,  600,  601,  604,\n",
       "          630,  644,  663,  666,  671,  682,  702,  711,  712,  749,  777,  778,\n",
       "          779,  803,  809,  822,  830,  850,  880,  907,  921,  925,  935,  955,\n",
       "          973,  992,  993, 1027, 1074, 1076, 1079, 1085, 1163, 1184, 1190, 1218,\n",
       "         1222, 1224, 1258, 1285, 1291, 1333, 1351, 1352, 1359, 1362, 1370, 1372,\n",
       "         1377, 1387, 1396, 1399, 1402, 1420, 1421, 1464, 1482, 1485, 1501, 1525,\n",
       "         1527, 1535, 1539, 1587, 1592, 1616, 1637, 1652, 1660, 1668, 1675, 1676,\n",
       "         1683, 1792, 1849, 1850, 1914, 1915, 1916, 1917, 1918, 1919, 1972, 1973,\n",
       "         1975, 2050, 2069, 2070, 2100, 2101, 2109, 2110, 2111, 2116, 2132, 2134,\n",
       "         2151, 2153, 2154, 2155, 2195, 2227, 2228, 2229, 2274, 2278, 2293, 2312,\n",
       "         2388, 2394, 2395, 2396, 2397, 2423, 2427, 2428, 2457, 2458, 2480, 2485,\n",
       "         2638, 2649]),\n",
       " tensor([ 780, 2341]),\n",
       " tensor([786, 947]),\n",
       " tensor([  20,   35,   47,  107,  118,  119,  125,  128,  132,  163,  193,  233,\n",
       "          238,  255,  264,  360,  370,  371,  379,  388,  392,  400,  440,  446,\n",
       "          448,  458,  468,  472,  541,  554,  559,  573,  597,  643,  646,  647,\n",
       "          685,  694,  703,  709,  720,  725,  728,  793,  795,  801,  844,  895,\n",
       "          903,  904,  940,  941,  971,  989, 1022, 1029, 1070, 1104, 1113, 1122,\n",
       "         1137, 1167, 1168, 1182, 1183, 1187, 1188, 1267, 1276, 1296, 1311, 1312,\n",
       "         1313, 1343, 1354, 1360, 1368, 1404, 1408, 1409, 1414, 1415, 1470, 1478,\n",
       "         1507, 1549, 1553, 1555, 1578, 1579, 1619, 1645, 1650, 1690, 1824, 1825,\n",
       "         1826, 1827, 1828, 1829, 1896, 1897, 1899, 1901, 1902, 1903, 1904, 1910,\n",
       "         1911, 1913, 1953, 1959, 1960, 1961, 1962, 1963, 2029, 2030, 2031, 2112,\n",
       "         2165, 2166, 2179, 2264, 2269, 2270, 2284, 2285, 2286, 2316, 2363, 2374,\n",
       "         2375, 2407, 2436, 2491, 2497, 2510, 2516, 2517, 2553, 2592, 2621]),\n",
       " tensor([ 135,  137,  397,  411,  548,  792,  799,  843,  879,  944,  961, 1014,\n",
       "         1191, 1465, 1488, 1523, 1558, 1728, 1937, 1938, 2095, 2138, 2144, 2145,\n",
       "         2257, 2329, 2331, 2338, 2339, 2340, 2449, 2504, 2542, 2555, 2599, 2601,\n",
       "         2609]),\n",
       " tensor([ 832, 2600]),\n",
       " tensor([  19,   40,  150,  219,  251,  253,  254,  295,  310,  336,  338,  352,\n",
       "          402,  507,  605,  729,  776,  812,  866,  872,  875,  877,  892,  914,\n",
       "          918,  981,  990, 1049, 1054, 1098, 1099, 1107, 1159, 1176, 1177, 1186,\n",
       "         1211, 1221, 1241, 1272, 1300, 1334, 1340, 1363, 1364, 1413, 1432, 1477,\n",
       "         1542, 1577, 1581, 1605, 1607, 1815, 1819, 1931, 1932, 1933, 1934, 1935,\n",
       "         1936, 1939, 1940, 1941, 1942, 1943, 1944, 1945, 1946, 1947, 1948, 1949,\n",
       "         1950, 2065, 2066, 2067, 2508, 2534, 2535, 2574, 2610, 2663]),\n",
       " tensor([ 917, 2639]),\n",
       " tensor([ 939, 2173, 2174]),\n",
       " tensor([ 953, 2565, 2566, 2567]),\n",
       " tensor([ 959, 2529]),\n",
       " tensor([ 967, 2659]),\n",
       " tensor([ 974, 1496]),\n",
       " tensor([ 986, 2697]),\n",
       " tensor([1005, 1541]),\n",
       " tensor([1059, 1600]),\n",
       " tensor([ 394,  546,  577,  722,  849,  870,  906, 1063, 1064, 1092, 1106, 1111,\n",
       "         1119, 1129, 1235, 1273, 1321, 1462, 1518, 1522, 1567, 1608, 1611, 1663,\n",
       "         1697, 1874, 1993, 2124, 2125, 2126, 2127, 2128, 2129, 2253, 2351, 2352,\n",
       "         2353, 2354, 2489, 2500, 2501, 2577, 2594, 2680]),\n",
       " tensor([ 727, 1151, 1386, 1877, 2570, 2571, 2572, 2573, 2700]),\n",
       " tensor([1210, 1648]),\n",
       " tensor([1233, 2433]),\n",
       " tensor([1236, 2479]),\n",
       " tensor([1263, 1407]),\n",
       " tensor([1298, 2703]),\n",
       " tensor([1310, 2692]),\n",
       " tensor([1356, 1613]),\n",
       " tensor([1371, 1393]),\n",
       " tensor([1375, 2586]),\n",
       " tensor([1438, 2664]),\n",
       " tensor([1554, 1657, 2686, 2687, 2688]),\n",
       " tensor([1563, 2633]),\n",
       " tensor([1673, 2660]),\n",
       " tensor([   4,    8,  101,  186,  237,  269,  281,  298,  319,  321,  327,  418,\n",
       "          500,  561,  575,  595,  624,  669,  746,  916,  928,  972,  979,  982,\n",
       "         1000, 1016, 1026, 1073, 1075, 1083, 1086, 1091, 1095, 1124, 1200, 1205,\n",
       "         1225, 1228, 1232, 1256, 1303, 1319, 1325, 1326, 1355, 1382, 1450, 1458,\n",
       "         1502, 1503, 1536, 1595, 1672, 1761, 1957, 1996, 2006, 2063, 2064, 2163,\n",
       "         2175, 2176, 2207, 2241, 2242, 2243, 2244, 2245, 2246, 2247, 2283, 2477,\n",
       "         2531, 2539, 2540, 2543, 2551, 2579, 2580, 2613, 2630, 2632, 2699, 2702]),\n",
       " tensor([  15,  395,  398,  599,  640,  730,  765,  894, 1090, 1093, 1147, 1271,\n",
       "         1598, 2177, 2367, 2368, 2369, 2370, 2371, 2420]),\n",
       " tensor([2410, 2411]),\n",
       " tensor([2431, 2432]),\n",
       " tensor([2602, 2603]),\n",
       " tensor([2618, 2619]),\n",
       " tensor([2625, 2626]),\n",
       " tensor([2634, 2635, 2636, 2693]),\n",
       " tensor([2665, 2666])]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rows = []\n",
    "cols = torch.arange(community_masks.size(1))\n",
    "for col in cols:\n",
    "    row = (community_masks[:, col] == 1).nonzero().view(-1)\n",
    "    rows.append(row)\n",
    "\n",
    "rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0.0000e+00, 0.0000e+00, 0.0000e+00, 4.8376e-03, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 4.0363e-03, 0.0000e+00,\n",
       "         3.5450e-02, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 3.5845e-02, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 4.1780e-03, 5.2496e-03, 0.0000e+00, 6.8465e-04,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         2.1806e-02, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.2987e-03,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 2.8890e-03, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         1.6872e-03, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.0357e-02,\n",
       "         0.0000e+00, 5.8086e-03, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 6.1379e-03, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 8.6653e-03, 0.0000e+00, 1.4920e-02, 1.3466e-02, 1.6147e-02,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 1.7778e-03, 0.0000e+00, 1.2979e-02,\n",
       "         9.6136e-03, 0.0000e+00, 0.0000e+00, 1.5376e-02, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 2.1474e-02, 0.0000e+00, 1.9768e-02,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 2.5218e-02,\n",
       "         1.1878e-02, 2.5314e-02, 0.0000e+00, 0.0000e+00, 1.5005e-02, 0.0000e+00,\n",
       "         2.7538e-02, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         9.2162e-03, 0.0000e+00, 2.8792e-03, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 2.5307e-03, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 7.3431e-03, 0.0000e+00, 0.0000e+00, 1.4487e-02,\n",
       "         0.0000e+00, 0.0000e+00, 7.4068e-03, 1.3745e-02, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.1470e-02, 0.0000e+00,\n",
       "         1.2860e-02, 0.0000e+00, 0.0000e+00, 0.0000e+00, 7.7861e-04, 1.2760e-02,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 2.0932e-02, 7.9081e-03, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 7.1406e-03, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 7.3507e-03, 0.0000e+00, 1.5938e-02,\n",
       "         0.0000e+00, 0.0000e+00, 4.6667e-03, 0.0000e+00, 0.0000e+00, 7.4147e-03,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 5.0464e-03,\n",
       "         1.5393e-02, 0.0000e+00, 0.0000e+00, 0.0000e+00, 2.2891e-05, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 3.2384e-04, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         1.3258e-05, 0.0000e+00, 0.0000e+00, 0.0000e+00, 5.7255e-03, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         2.1790e-02, 1.0384e-03, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 1.0141e-02, 0.0000e+00, 7.5603e-03,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 3.0837e-02, 1.5398e-02,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 7.7450e-03, 6.1564e-03,\n",
       "         9.2593e-03, 0.0000e+00, 0.0000e+00, 2.5466e-03, 8.9865e-03, 0.0000e+00,\n",
       "         5.6021e-03, 0.0000e+00, 1.2929e-02, 1.1825e-03, 0.0000e+00, 1.8550e-02,\n",
       "         0.0000e+00, 3.8243e-03, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 5.0856e-03, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.2928e-02, 1.9800e-02,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 7.5618e-03, 1.1132e-02, 5.9690e-03,\n",
       "         1.4555e-02, 0.0000e+00, 0.0000e+00, 3.3836e-03, 0.0000e+00, 5.4321e-03,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 3.1913e-02, 5.6366e-03,\n",
       "         3.1437e-02, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 5.6978e-04,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 9.6951e-03, 1.6724e-02, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.2557e-02, 4.6454e-03,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         1.9625e-02, 0.0000e+00, 0.0000e+00, 1.0388e-02, 0.0000e+00, 0.0000e+00,\n",
       "         1.0806e-02, 1.0653e-02, 0.0000e+00, 0.0000e+00, 1.4727e-02, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 3.9920e-03, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 5.3466e-03, 2.5009e-03, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 6.3355e-03,\n",
       "         0.0000e+00, 0.0000e+00, 3.9964e-03, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         7.0904e-03, 5.7939e-03, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 7.0495e-03, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 1.9755e-03, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         3.3637e-03, 0.0000e+00, 1.5008e-02, 3.3171e-03, 0.0000e+00, 2.0645e-02,\n",
       "         2.4578e-02, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 3.0282e-02,\n",
       "         0.0000e+00, 0.0000e+00, 1.4522e-02, 0.0000e+00, 0.0000e+00, 1.5128e-02,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 6.9316e-03, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         2.6120e-03, 1.7473e-03, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.4149e-03,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 3.7590e-03, 2.6614e-03, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 2.3702e-02, 0.0000e+00,\n",
       "         1.3298e-02, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         1.7320e-02, 1.2153e-02, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 1.6381e-02, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 1.4927e-02, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 5.2450e-03, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 1.3257e-03, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 4.8582e-03, 0.0000e+00, 2.9342e-03,\n",
       "         0.0000e+00, 0.0000e+00, 2.0125e-03, 0.0000e+00, 4.4722e-03, 0.0000e+00,\n",
       "         4.3582e-03, 0.0000e+00], grad_fn=<SliceBackward0>),\n",
       " tensor([0.0000e+00, 0.0000e+00, 0.0000e+00, 4.8376e-03, 1.1651e-02, 1.1191e-02,\n",
       "         0.0000e+00, 1.1178e-02, 1.7264e-02, 0.0000e+00, 4.0363e-03, 1.8698e-02,\n",
       "         3.5450e-02, 0.0000e+00, 9.5916e-03, 0.0000e+00, 0.0000e+00, 1.7136e-03,\n",
       "         0.0000e+00, 3.5845e-02, 1.9300e-02, 0.0000e+00, 1.4359e-02, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 3.6402e-03, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 5.2496e-03, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 6.6092e-03, 0.0000e+00, 7.9253e-03, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 8.4226e-03, 0.0000e+00, 0.0000e+00, 1.2987e-03,\n",
       "         0.0000e+00, 0.0000e+00, 4.1715e-03, 0.0000e+00, 0.0000e+00, 1.4694e-02,\n",
       "         0.0000e+00, 2.8890e-03, 6.0022e-03, 8.5667e-04, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 3.7604e-02, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 5.8086e-03, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         1.1165e-02, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.8321e-02, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 6.1379e-03, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 8.6653e-03, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.6147e-02,\n",
       "         0.0000e+00, 1.4061e-02, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 1.5376e-02, 2.1061e-02, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 2.1474e-02, 0.0000e+00, 1.9768e-02,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 1.1598e-02, 0.0000e+00, 2.5218e-02,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.5005e-02, 0.0000e+00,\n",
       "         2.7538e-02, 0.0000e+00, 4.8239e-03, 0.0000e+00, 0.0000e+00, 2.0387e-02,\n",
       "         0.0000e+00, 1.5047e-02, 2.8792e-03, 0.0000e+00, 1.5463e-02, 0.0000e+00,\n",
       "         0.0000e+00, 1.2293e-02, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.3975e-02,\n",
       "         0.0000e+00, 5.9315e-03, 7.3431e-03, 6.0342e-03, 0.0000e+00, 0.0000e+00,\n",
       "         1.2286e-02, 1.2077e-02, 0.0000e+00, 0.0000e+00, 0.0000e+00, 2.4406e-02,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 7.7861e-04, 1.2760e-02,\n",
       "         2.9551e-03, 0.0000e+00, 0.0000e+00, 2.0932e-02, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 2.4552e-02, 7.1406e-03, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 6.0337e-03, 0.0000e+00, 7.3507e-03, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 1.5144e-02, 0.0000e+00, 0.0000e+00, 9.9142e-03, 7.4147e-03,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.5369e-04, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 2.2891e-05, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 3.3745e-02, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 8.5002e-03, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         1.3258e-05, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.1682e-02,\n",
       "         0.0000e+00, 0.0000e+00, 7.8074e-03, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 1.0384e-03, 0.0000e+00, 0.0000e+00, 4.7891e-03, 0.0000e+00,\n",
       "         0.0000e+00, 1.1977e-02, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.5398e-02,\n",
       "         0.0000e+00, 0.0000e+00, 2.8029e-03, 0.0000e+00, 7.7450e-03, 0.0000e+00,\n",
       "         9.2593e-03, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         5.6021e-03, 0.0000e+00, 0.0000e+00, 1.1825e-03, 0.0000e+00, 1.8550e-02,\n",
       "         1.2133e-02, 3.8243e-03, 8.8860e-03, 0.0000e+00, 0.0000e+00, 3.5718e-03,\n",
       "         2.1472e-02, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.2928e-02, 1.9800e-02,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 7.5618e-03, 1.1132e-02, 5.9690e-03,\n",
       "         1.4555e-02, 1.6362e-02, 0.0000e+00, 0.0000e+00, 0.0000e+00, 5.4321e-03,\n",
       "         0.0000e+00, 0.0000e+00, 8.7336e-03, 0.0000e+00, 3.1913e-02, 0.0000e+00,\n",
       "         0.0000e+00, 9.1752e-03, 3.2867e-03, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 9.6951e-03, 1.6724e-02, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 2.6807e-03, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 1.5314e-02, 0.0000e+00, 4.6454e-03,\n",
       "         0.0000e+00, 1.0464e-02, 0.0000e+00, 1.1317e-02, 0.0000e+00, 0.0000e+00,\n",
       "         1.9625e-02, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         1.0806e-02, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.4727e-02, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 3.9920e-03, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         3.4216e-02, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 6.3355e-03,\n",
       "         0.0000e+00, 0.0000e+00, 3.9964e-03, 0.0000e+00, 1.9705e-02, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 1.9323e-02, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 7.0495e-03, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 1.2651e-02, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 8.7687e-03, 0.0000e+00,\n",
       "         2.4578e-02, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 1.7968e-02, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         2.6120e-03, 0.0000e+00, 0.0000e+00, 0.0000e+00, 3.5779e-03, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 2.3702e-02, 0.0000e+00,\n",
       "         1.3298e-02, 9.3405e-04, 0.0000e+00, 9.8556e-03, 5.9980e-03, 0.0000e+00,\n",
       "         1.7320e-02, 1.2153e-02, 0.0000e+00, 0.0000e+00, 5.3820e-03, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 8.6408e-03,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.3756e-02,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 5.1718e-03,\n",
       "         4.6664e-03, 0.0000e+00, 0.0000e+00, 1.3257e-03, 1.5218e-02, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 4.8582e-03, 0.0000e+00, 2.9342e-03,\n",
       "         0.0000e+00, 0.0000e+00, 2.0125e-03, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         4.3582e-03, 0.0000e+00], grad_fn=<SliceBackward0>),\n",
       " torch.Size([512]),\n",
       " torch.Size([512]))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[rows[-1][0],:], x[rows[-1][1],:], x[rows[-1][0],:].shape, x[rows[-1][1],:].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
       "         0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
       "         0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
       "         0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 1.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 1., 0., 0., 0.,\n",
       "         1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0.,\n",
       "         0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 1., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,\n",
       "         0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
       "         0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 1., 0., 0., 0., 0., 0.], grad_fn=<SumBackward1>),\n",
       " tensor([1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1.,\n",
       "         1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1.,\n",
       "         1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0.,\n",
       "         1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 0.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 0., 1., 1., 1.,\n",
       "         0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1.,\n",
       "         1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 0., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1.,\n",
       "         1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0.,\n",
       "         1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 0., 1., 1., 1., 1., 1.], grad_fn=<RsubBackward1>),\n",
       " torch.Size([512]))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input1,input2 = x[rows[-2][0],:].unsqueeze(dim=1), x[rows[-1][1],:].unsqueeze(dim=1)\n",
    "\n",
    "simi_tensor = F.cosine_similarity(input1,input2, dim=1, eps=1e-8)\n",
    "dist_tensor = 1 - simi_tensor\n",
    "\n",
    "simi_tensor, dist_tensor, simi_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.0918, grad_fn=<MeanBackward0>),\n",
       " tensor(0.9082, grad_fn=<MeanBackward0>))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.mean(simi_tensor), torch.mean(dist_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(3), tensor(4))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.y[rows[-2][0]], data.y[rows[-1][1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GCN(\n",
       "  (conv1): GCNConv(1433, 512)\n",
       "  (conv2): GCNConv(512, 7)\n",
       ")"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer = torch.optim.Adam(model.parameters(), lr=0.005, weight_decay=5e-4)\n",
    "# scheduler = StepLR(optimizer, step_size=30, gamma=0.5)\n",
    "# cross_entropy = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[   0,    1,    2,  ..., 2705, 2706, 2707]]), torch.Size([1, 2708]))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "node_idx = torch.arange(data.x.size(0)).unsqueeze(dim=0)\n",
    "node_idx, node_idx.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2708, 104])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "community_masks.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize(h, color):\n",
    "    z = TSNE(n_components=2).fit_transform(h.detach().cpu().numpy())\n",
    "\n",
    "    plt.figure(figsize=(10,10))\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "\n",
    "    plt.scatter(z[:, 0], z[:, 1], s=70, c=color, cmap=\"Set2\")\n",
    "    plt.savefig('figs/cora_1025.png')\n",
    "\n",
    "\n",
    "def accuracy(y_pred, y_true):\n",
    "    \n",
    "    correct = y_pred.eq(y_true).double()\n",
    "    correct = correct.sum().item()\n",
    "    return correct / len(y_true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 关键思路：\n",
    "1. 找到节点所属社团（在community_mask里通过行号关联找到非零列号）；\n",
    "2. 计算节点表示和（根据非零列号索引集合对应的）相同社团节点表示两者的相似性和距离，同时可以计算不同社团之间的相似性和距离，我们希望相同社团的相似性高，不同社团对应的相似性低；\n",
    "3. 基于此构造loss函数中的归一化项，然后结合交叉熵用于节点分类，实现模型的训练和学习；\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "社团图的节点： [0, 1]\n",
      "社团之间的距离： {0: {0: 0, 1: 1}, 1: {1: 0, 0: 1}}\n",
      "对应最远社团： {0: 1, 1: 0}\n"
     ]
    }
   ],
   "source": [
    "# 先构建社团图\n",
    "def construct_community_graph(edges, community_assignments):\n",
    "\n",
    "    # 1. 创建新的社团图\n",
    "    community_graph = nx.Graph()\n",
    "\n",
    "    # 2. 添加社团节点\n",
    "    for community_id in set(community_assignments.values()):\n",
    "        community_graph.add_node(community_id)\n",
    "\n",
    "    # 3. 构建社团之间的连接关系\n",
    "    for edge in edges:\n",
    "        node1, node2 = edge\n",
    "        community1, community2 =community_assignments.get(node1, None), community_assignments.get(node2, None)\n",
    "\n",
    "        # 确保节点属于哪个社团\n",
    "        if community1 is not None and community2 is not None:\n",
    "            if community1 != community2:\n",
    "                # 社团1和社团2之间没有边，添加一条边\n",
    "                if not community_graph.has_edge(community1, community2):\n",
    "                    community_graph.add_edge(community1, community2)\n",
    "\n",
    "    # 这里采用社团图的最短路径距离\n",
    "    community_distances = dict(nx.all_pairs_shortest_path_length(community_graph))\n",
    "\n",
    "    # 对于每个社团，根据最短距离选取离得最远的社团并返回各自对应的社团编号\n",
    "    max_dis_communities = {}\n",
    "    for com in community_distances:\n",
    "        sorted_items = sorted(community_distances[com].items(), key=lambda item: item[1], reverse=True)\n",
    "        max_com = sorted_items[0][0]\n",
    "        max_dis_communities[com] = max_com\n",
    "\n",
    "    return community_graph, community_distances, max_dis_communities\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 示例数据\n",
    "edges = [(1, 2), (1, 3), (2, 4), (2, 3), (4, 5), (4, 6), (5, 6), (7, 8)]\n",
    "community_assignments = {1: 0, 2: 0, 3: 0, 4: 1, 5: 1, 6: 1}\n",
    "\n",
    "# 构建社团图和计算社团之间的距离\n",
    "community_graph, community_distances, max_dis_communities = construct_community_graph(edges, community_assignments)\n",
    "\n",
    "\n",
    "\n",
    "# 输出社团图和社团之间的距离\n",
    "print(\"社团图的节点：\", community_graph.nodes)\n",
    "print(\"社团之间的距离：\", community_distances)\n",
    "print(\"对应最远社团：\", max_dis_communities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(0, 633),\n",
       " (0, 1862),\n",
       " (0, 2582),\n",
       " (633, 1701),\n",
       " (633, 1866),\n",
       " (1862, 926),\n",
       " (1862, 1701),\n",
       " (1862, 2582),\n",
       " (2582, 1166),\n",
       " (1, 2),\n",
       " (1, 652),\n",
       " (1, 654),\n",
       " (2, 332),\n",
       " (2, 1454),\n",
       " (2, 1666),\n",
       " (2, 1986),\n",
       " (652, 470),\n",
       " (332, 665),\n",
       " (332, 2003),\n",
       " (332, 2122),\n",
       " (332, 2615),\n",
       " (1666, 48),\n",
       " (1666, 49),\n",
       " (1666, 606),\n",
       " (1666, 1662),\n",
       " (1666, 2381),\n",
       " (1986, 45),\n",
       " (1986, 68),\n",
       " (1986, 71),\n",
       " (1986, 151),\n",
       " (1986, 160),\n",
       " (1986, 179),\n",
       " (1986, 201),\n",
       " (1986, 215),\n",
       " (1986, 232),\n",
       " (1986, 335),\n",
       " (1986, 366),\n",
       " (1986, 391),\n",
       " (1986, 460),\n",
       " (1986, 476),\n",
       " (1986, 519),\n",
       " (1986, 566),\n",
       " (1986, 673),\n",
       " (1986, 681),\n",
       " (1986, 743),\n",
       " (1986, 745),\n",
       " (1986, 792),\n",
       " (1986, 899),\n",
       " (1986, 968),\n",
       " (1986, 1023),\n",
       " (1986, 1095),\n",
       " (1986, 1127),\n",
       " (1986, 1149),\n",
       " (1986, 1166),\n",
       " (1986, 1434),\n",
       " (1986, 1453),\n",
       " (1986, 1558),\n",
       " (1986, 1574),\n",
       " (1986, 1697),\n",
       " (1986, 1704),\n",
       " (1986, 1709),\n",
       " (1986, 1812),\n",
       " (1986, 1859),\n",
       " (1986, 1870),\n",
       " (1986, 1873),\n",
       " (1986, 1875),\n",
       " (1986, 1876),\n",
       " (1986, 1987),\n",
       " (1986, 1988),\n",
       " (1986, 1989),\n",
       " (1986, 1990),\n",
       " (1986, 1991),\n",
       " (1986, 1992),\n",
       " (1986, 1993),\n",
       " (1986, 1994),\n",
       " (1986, 1995),\n",
       " (1986, 1996),\n",
       " (1986, 1997),\n",
       " (1986, 1998),\n",
       " (1986, 1999),\n",
       " (1986, 2000),\n",
       " (1986, 2001),\n",
       " (1986, 2002),\n",
       " (1986, 2003),\n",
       " (1986, 2004),\n",
       " (1986, 2005),\n",
       " (1986, 2006),\n",
       " (1986, 2007),\n",
       " (1986, 2008),\n",
       " (1986, 2009),\n",
       " (3, 2544),\n",
       " (4, 1016),\n",
       " (4, 1256),\n",
       " (4, 1761),\n",
       " (4, 2175),\n",
       " (4, 2176),\n",
       " (1016, 561),\n",
       " (1016, 595),\n",
       " (1016, 1256),\n",
       " (1016, 2176),\n",
       " (1256, 595),\n",
       " (1256, 982),\n",
       " (1256, 1091),\n",
       " (1256, 1761),\n",
       " (1256, 2175),\n",
       " (1256, 2176),\n",
       " (1761, 223),\n",
       " (1761, 1205),\n",
       " (1761, 1358),\n",
       " (1761, 1721),\n",
       " (1761, 2175),\n",
       " (2175, 982),\n",
       " (2175, 2176),\n",
       " (2176, 595),\n",
       " (2176, 982),\n",
       " (2176, 1091),\n",
       " (2176, 1382),\n",
       " (5, 1629),\n",
       " (5, 1659),\n",
       " (5, 2546),\n",
       " (1629, 1659),\n",
       " (1629, 1711),\n",
       " (2546, 466),\n",
       " (2546, 628),\n",
       " (2546, 952),\n",
       " (6, 373),\n",
       " (6, 1042),\n",
       " (6, 1416),\n",
       " (6, 1602),\n",
       " (373, 1025),\n",
       " (373, 1042),\n",
       " (1042, 74),\n",
       " (1042, 485),\n",
       " (1042, 624),\n",
       " (1042, 901),\n",
       " (1042, 1025),\n",
       " (1042, 1047),\n",
       " (1042, 1118),\n",
       " (1042, 1125),\n",
       " (1042, 1198),\n",
       " (1042, 1481),\n",
       " (1042, 1517),\n",
       " (1042, 1628),\n",
       " (1042, 1925),\n",
       " (1042, 1926),\n",
       " (1042, 2051),\n",
       " (1042, 2052),\n",
       " (1042, 2054),\n",
       " (1042, 2055),\n",
       " (1042, 2073),\n",
       " (1042, 2198),\n",
       " (1042, 2333),\n",
       " (1416, 30),\n",
       " (1416, 61),\n",
       " (1416, 74),\n",
       " (1416, 149),\n",
       " (1416, 305),\n",
       " (1416, 572),\n",
       " (1416, 718),\n",
       " (1416, 1008),\n",
       " (1416, 1118),\n",
       " (1416, 1468),\n",
       " (1416, 1602),\n",
       " (1416, 1921),\n",
       " (1416, 1922),\n",
       " (1416, 1923),\n",
       " (1416, 1924),\n",
       " (1416, 1925),\n",
       " (1416, 1926),\n",
       " (1602, 95),\n",
       " (1602, 100),\n",
       " (1602, 315),\n",
       " (1602, 1204),\n",
       " (1602, 2054),\n",
       " (1602, 2072),\n",
       " (1602, 2073),\n",
       " (1602, 2074),\n",
       " (7, 208),\n",
       " (8, 269),\n",
       " (8, 281),\n",
       " (8, 1996),\n",
       " (269, 321),\n",
       " (269, 418),\n",
       " (269, 2543),\n",
       " (269, 2551),\n",
       " (281, 101),\n",
       " (281, 746),\n",
       " (281, 1000),\n",
       " (281, 1347),\n",
       " (281, 1382),\n",
       " (281, 2244),\n",
       " (281, 2247),\n",
       " (1996, 327),\n",
       " (1996, 2063),\n",
       " (1996, 2064),\n",
       " (9, 723),\n",
       " (9, 2614),\n",
       " (723, 2614),\n",
       " (2614, 494),\n",
       " (2614, 1599),\n",
       " (2614, 2442),\n",
       " (10, 476),\n",
       " (10, 2545),\n",
       " (476, 306),\n",
       " (476, 1140),\n",
       " (476, 1800),\n",
       " (11, 1655),\n",
       " (11, 1839),\n",
       " (1655, 149),\n",
       " (1655, 1121),\n",
       " (1655, 1131),\n",
       " (1655, 1410),\n",
       " (1655, 1839),\n",
       " (1655, 1842),\n",
       " (1655, 1894),\n",
       " (1655, 2136),\n",
       " (1655, 2282),\n",
       " (1655, 2295),\n",
       " (1655, 2384),\n",
       " (1839, 979),\n",
       " (1839, 1204),\n",
       " (1839, 2424),\n",
       " (1839, 2453),\n",
       " (12, 1001),\n",
       " (12, 1318),\n",
       " (12, 2661),\n",
       " (12, 2662),\n",
       " (1001, 2662),\n",
       " (1318, 2661),\n",
       " (1318, 2662),\n",
       " (13, 1701),\n",
       " (13, 1810),\n",
       " (1701, 24),\n",
       " (1701, 143),\n",
       " (1701, 157),\n",
       " (1701, 158),\n",
       " (1701, 201),\n",
       " (1701, 205),\n",
       " (1701, 215),\n",
       " (1701, 216),\n",
       " (1701, 226),\n",
       " (1701, 231),\n",
       " (1701, 232),\n",
       " (1701, 235),\n",
       " (1701, 261),\n",
       " (1701, 318),\n",
       " (1701, 335),\n",
       " (1701, 343),\n",
       " (1701, 416),\n",
       " (1701, 467),\n",
       " (1701, 532),\n",
       " (1701, 547),\n",
       " (1701, 563),\n",
       " (1701, 664),\n",
       " (1701, 699),\n",
       " (1701, 729),\n",
       " (1701, 767),\n",
       " (1701, 784),\n",
       " (1701, 869),\n",
       " (1701, 874),\n",
       " (1701, 1023),\n",
       " (1701, 1052),\n",
       " (1701, 1075),\n",
       " (1701, 1100),\n",
       " (1701, 1212),\n",
       " (1701, 1241),\n",
       " (1701, 1299),\n",
       " (1701, 1323),\n",
       " (1701, 1334),\n",
       " (1701, 1337),\n",
       " (1701, 1440),\n",
       " (1701, 1453),\n",
       " (1701, 1479),\n",
       " (1701, 1636),\n",
       " (1701, 1665),\n",
       " (1701, 1799),\n",
       " (1701, 1820),\n",
       " (1701, 1846),\n",
       " (1701, 1852),\n",
       " (1701, 1853),\n",
       " (1701, 1854),\n",
       " (1701, 1855),\n",
       " (1701, 1856),\n",
       " (1701, 1857),\n",
       " (1701, 1858),\n",
       " (1701, 1859),\n",
       " (1701, 1860),\n",
       " (1701, 1861),\n",
       " (1701, 1863),\n",
       " (1701, 1864),\n",
       " (1701, 1865),\n",
       " (1701, 1866),\n",
       " (1701, 1867),\n",
       " (1701, 1868),\n",
       " (1701, 1869),\n",
       " (1701, 1870),\n",
       " (1701, 1871),\n",
       " (1701, 1872),\n",
       " (1701, 1873),\n",
       " (1701, 1874),\n",
       " (1701, 1875),\n",
       " (1701, 1876),\n",
       " (1701, 1877),\n",
       " (1810, 27),\n",
       " (1810, 224),\n",
       " (1810, 230),\n",
       " (1810, 351),\n",
       " (1810, 481),\n",
       " (1810, 482),\n",
       " (1810, 511),\n",
       " (1810, 549),\n",
       " (1810, 568),\n",
       " (1810, 576),\n",
       " (1810, 716),\n",
       " (1810, 719),\n",
       " (1810, 790),\n",
       " (1810, 795),\n",
       " (1810, 835),\n",
       " (1810, 962),\n",
       " (1810, 1004),\n",
       " (1810, 1062),\n",
       " (1810, 1095),\n",
       " (1810, 1107),\n",
       " (1810, 1121),\n",
       " (1810, 1299),\n",
       " (1810, 1331),\n",
       " (1810, 1348),\n",
       " (1810, 1576),\n",
       " (1810, 1581),\n",
       " (1810, 1787),\n",
       " (1810, 1808),\n",
       " (1810, 1809),\n",
       " (1810, 1811),\n",
       " (1810, 1812),\n",
       " (1810, 1813),\n",
       " (1810, 1814),\n",
       " (1810, 1815),\n",
       " (1810, 1816),\n",
       " (1810, 1817),\n",
       " (1810, 1818),\n",
       " (1810, 1819),\n",
       " (1810, 1820),\n",
       " (1810, 1821),\n",
       " (1810, 1822),\n",
       " (1810, 1823),\n",
       " (1810, 1869),\n",
       " (14, 158),\n",
       " (14, 2034),\n",
       " (14, 2075),\n",
       " (14, 2077),\n",
       " (14, 2668),\n",
       " (158, 180),\n",
       " (158, 2034),\n",
       " (2034, 49),\n",
       " (2034, 86),\n",
       " (2034, 141),\n",
       " (2034, 224),\n",
       " (2034, 382),\n",
       " (2034, 417),\n",
       " (2034, 617),\n",
       " (2034, 691),\n",
       " (2034, 791),\n",
       " (2034, 841),\n",
       " (2034, 1002),\n",
       " (2034, 1026),\n",
       " (2034, 1270),\n",
       " (2034, 1273),\n",
       " (2034, 1319),\n",
       " (2034, 1336),\n",
       " (2034, 1448),\n",
       " (2034, 1497),\n",
       " (2034, 1614),\n",
       " (2034, 1654),\n",
       " (2034, 1665),\n",
       " (2034, 1807),\n",
       " (2034, 1812),\n",
       " (2034, 1889),\n",
       " (2034, 1891),\n",
       " (2034, 1894),\n",
       " (2034, 1929),\n",
       " (2034, 1968),\n",
       " (2034, 1983),\n",
       " (2034, 2035),\n",
       " (2034, 2036),\n",
       " (2034, 2037),\n",
       " (2034, 2038),\n",
       " (2034, 2039),\n",
       " (2034, 2040),\n",
       " (2034, 2041),\n",
       " (2034, 2042),\n",
       " (2034, 2130),\n",
       " (2075, 775),\n",
       " (2075, 1020),\n",
       " (2075, 2076),\n",
       " (2075, 2077),\n",
       " (2075, 2091),\n",
       " (2075, 2667),\n",
       " (2075, 2668),\n",
       " (2077, 486),\n",
       " (2077, 1020),\n",
       " (2077, 1704),\n",
       " (2077, 2076),\n",
       " (2668, 45),\n",
       " (2668, 486),\n",
       " (2668, 2667),\n",
       " (15, 1090),\n",
       " (15, 1093),\n",
       " (15, 1271),\n",
       " (15, 2367),\n",
       " (1090, 640),\n",
       " (1090, 1093),\n",
       " (1090, 1147),\n",
       " (1090, 1271),\n",
       " (1090, 1598),\n",
       " (1090, 2367),\n",
       " (1093, 599),\n",
       " (1093, 1271),\n",
       " (1093, 1598),\n",
       " (1093, 2367),\n",
       " (1271, 599),\n",
       " (1271, 2367),\n",
       " (2367, 395),\n",
       " (2367, 765),\n",
       " (2367, 894),\n",
       " (2367, 1147),\n",
       " (2367, 1598),\n",
       " (2367, 2177),\n",
       " (2367, 2368),\n",
       " (2367, 2369),\n",
       " (2367, 2370),\n",
       " (2367, 2371),\n",
       " (16, 970),\n",
       " (16, 1632),\n",
       " (16, 2444),\n",
       " (16, 2642),\n",
       " (970, 364),\n",
       " (970, 466),\n",
       " (970, 1358),\n",
       " (970, 2365),\n",
       " (1632, 364),\n",
       " (1632, 1756),\n",
       " (2444, 464),\n",
       " (2444, 580),\n",
       " (2444, 1710),\n",
       " (2444, 1742),\n",
       " (2444, 2503),\n",
       " (2642, 358),\n",
       " (2642, 364),\n",
       " (2642, 1756),\n",
       " (17, 24),\n",
       " (17, 927),\n",
       " (17, 1315),\n",
       " (17, 1316),\n",
       " (17, 2140),\n",
       " (24, 201),\n",
       " (24, 598),\n",
       " (24, 1636),\n",
       " (24, 2139),\n",
       " (24, 2141),\n",
       " (927, 1316),\n",
       " (927, 2140),\n",
       " (1315, 1301),\n",
       " (1316, 1301),\n",
       " (1316, 1679),\n",
       " (2140, 1301),\n",
       " (2140, 1679),\n",
       " (2140, 2139),\n",
       " (18, 139),\n",
       " (18, 1560),\n",
       " (18, 1786),\n",
       " (18, 2082),\n",
       " (18, 2145),\n",
       " (139, 103),\n",
       " (139, 306),\n",
       " (139, 660),\n",
       " (139, 910),\n",
       " (139, 1623),\n",
       " (139, 1780),\n",
       " (139, 2045),\n",
       " (1560, 505),\n",
       " (1560, 1623),\n",
       " (1786, 1564),\n",
       " (1786, 1624),\n",
       " (2082, 133),\n",
       " (2145, 1512),\n",
       " (2145, 2095),\n",
       " (2145, 2144),\n",
       " (19, 1939),\n",
       " (1939, 1542),\n",
       " (20, 1072),\n",
       " (20, 2269),\n",
       " (20, 2270),\n",
       " (20, 2374),\n",
       " (20, 2375),\n",
       " (1072, 189),\n",
       " (1072, 236),\n",
       " (1072, 244),\n",
       " (1072, 306),\n",
       " (1072, 342),\n",
       " (1072, 417),\n",
       " (1072, 773),\n",
       " (1072, 945),\n",
       " (1072, 958),\n",
       " (1072, 973),\n",
       " (1072, 1070),\n",
       " (1072, 1262),\n",
       " (1072, 1358),\n",
       " (1072, 1478),\n",
       " (1072, 1483),\n",
       " (1072, 1505),\n",
       " (1072, 1725),\n",
       " (1072, 1733),\n",
       " (1072, 1740),\n",
       " (1072, 1784),\n",
       " (1072, 1797),\n",
       " (1072, 1798),\n",
       " (1072, 1799),\n",
       " (1072, 1800),\n",
       " (1072, 1801),\n",
       " (1072, 1802),\n",
       " (1072, 1803),\n",
       " (1072, 1804),\n",
       " (1072, 1805),\n",
       " (2269, 370),\n",
       " (2269, 392),\n",
       " (2269, 1415),\n",
       " (2269, 1619),\n",
       " (2269, 1903),\n",
       " (2269, 2270),\n",
       " (2270, 128),\n",
       " (2270, 1354),\n",
       " (2270, 1414),\n",
       " (2270, 2375),\n",
       " (2375, 371),\n",
       " (2375, 1404),\n",
       " (21, 1043),\n",
       " (21, 2310),\n",
       " (1043, 2309),\n",
       " (2310, 1920),\n",
       " (22, 39),\n",
       " (22, 1234),\n",
       " (22, 1702),\n",
       " (22, 1703),\n",
       " (22, 2238),\n",
       " (39, 1349),\n",
       " (39, 1522),\n",
       " (39, 1532),\n",
       " (39, 1634),\n",
       " (39, 1965),\n",
       " (39, 2357),\n",
       " (1234, 1702),\n",
       " (1234, 1703),\n",
       " (1234, 1966),\n",
       " (1702, 463),\n",
       " (1702, 1240),\n",
       " (1702, 1365),\n",
       " (1702, 1703),\n",
       " (1702, 1966),\n",
       " (1702, 1970),\n",
       " (1702, 1971),\n",
       " (1702, 2238),\n",
       " (1703, 463),\n",
       " (1703, 759),\n",
       " (1703, 789),\n",
       " (1703, 963),\n",
       " (1703, 1365),\n",
       " (1703, 1417),\n",
       " (1703, 1906),\n",
       " (1703, 1966),\n",
       " (1703, 1967),\n",
       " (1703, 1968),\n",
       " (1703, 1969),\n",
       " (1703, 1970),\n",
       " (1703, 1971),\n",
       " (1703, 2238),\n",
       " (2238, 151),\n",
       " (2238, 152),\n",
       " (2238, 627),\n",
       " (2238, 706),\n",
       " (2238, 1969),\n",
       " (2238, 2237),\n",
       " (2238, 2239),\n",
       " (2238, 2240),\n",
       " (23, 2159),\n",
       " (2159, 2157),\n",
       " (2159, 2160),\n",
       " (201, 297),\n",
       " (201, 570),\n",
       " (201, 598),\n",
       " (201, 2430),\n",
       " (598, 48),\n",
       " (598, 143),\n",
       " (598, 157),\n",
       " (598, 165),\n",
       " (598, 297),\n",
       " (598, 316),\n",
       " (598, 480),\n",
       " (598, 519),\n",
       " (598, 521),\n",
       " (598, 547),\n",
       " (598, 637),\n",
       " (598, 766),\n",
       " (598, 845),\n",
       " (598, 869),\n",
       " (598, 968),\n",
       " (598, 1003),\n",
       " (598, 1100),\n",
       " (598, 1107),\n",
       " (598, 1297),\n",
       " (598, 1299),\n",
       " (598, 1301),\n",
       " (598, 1473),\n",
       " (598, 1573),\n",
       " (598, 1636),\n",
       " (598, 1821),\n",
       " (598, 1823),\n",
       " (598, 1864),\n",
       " (598, 1870),\n",
       " (598, 1875),\n",
       " (598, 2138),\n",
       " (598, 2707),\n",
       " (1636, 2139),\n",
       " (1636, 2141),\n",
       " (2139, 2141),\n",
       " (2141, 2142),\n",
       " (25, 1301),\n",
       " (25, 1344),\n",
       " (25, 2011),\n",
       " (25, 2317),\n",
       " (1301, 1542),\n",
       " (1344, 724),\n",
       " (1344, 815),\n",
       " (1344, 1928),\n",
       " (1344, 2315),\n",
       " (2011, 88),\n",
       " (2011, 733),\n",
       " (2317, 1403),\n",
       " (2317, 1928),\n",
       " (26, 99),\n",
       " (26, 122),\n",
       " (26, 123),\n",
       " (26, 2454),\n",
       " (26, 2455),\n",
       " (99, 122),\n",
       " (99, 123),\n",
       " (99, 2454),\n",
       " (99, 2455),\n",
       " (99, 2604),\n",
       " (122, 2454),\n",
       " (122, 2455),\n",
       " (123, 2455),\n",
       " (123, 2604),\n",
       " (27, 606),\n",
       " (27, 2360),\n",
       " (27, 2578),\n",
       " (606, 531),\n",
       " (606, 2230),\n",
       " (606, 2360),\n",
       " (606, 2361),\n",
       " (2360, 2027),\n",
       " (2360, 2578),\n",
       " (28, 1687),\n",
       " (1687, 246),\n",
       " (1687, 1003),\n",
       " (1687, 1721),\n",
       " (29, 963),\n",
       " (29, 2645),\n",
       " (963, 43),\n",
       " (963, 203),\n",
       " (963, 258),\n",
       " (963, 375),\n",
       " (963, 706),\n",
       " (963, 805),\n",
       " (963, 1094),\n",
       " (963, 1141),\n",
       " (963, 1157),\n",
       " (963, 1417),\n",
       " (963, 1443),\n",
       " (963, 1785),\n",
       " (963, 2240),\n",
       " (963, 2399),\n",
       " (963, 2400),\n",
       " (963, 2401),\n",
       " (963, 2434),\n",
       " (2645, 258),\n",
       " (2645, 706),\n",
       " (2645, 1443),\n",
       " (30, 697),\n",
       " (30, 738),\n",
       " (30, 1358),\n",
       " (30, 2162),\n",
       " (30, 2343),\n",
       " (697, 1080),\n",
       " (738, 405),\n",
       " (738, 1080),\n",
       " (738, 1927),\n",
       " (738, 2162),\n",
       " (1358, 34),\n",
       " (1358, 53),\n",
       " (1358, 59),\n",
       " (1358, 68),\n",
       " (1358, 72),\n",
       " (1358, 73),\n",
       " (1358, 90),\n",
       " (1358, 101),\n",
       " (1358, 111),\n",
       " (1358, 154),\n",
       " (1358, 155),\n",
       " (1358, 156),\n",
       " (1358, 170),\n",
       " (1358, 173),\n",
       " (1358, 174),\n",
       " (1358, 228),\n",
       " (1358, 244),\n",
       " (1358, 246),\n",
       " (1358, 289),\n",
       " (1358, 326),\n",
       " (1358, 333),\n",
       " (1358, 337),\n",
       " (1358, 341),\n",
       " (1358, 346),\n",
       " (1358, 357),\n",
       " (1358, 364),\n",
       " (1358, 466),\n",
       " (1358, 489),\n",
       " (1358, 501),\n",
       " (1358, 524),\n",
       " (1358, 552),\n",
       " (1358, 580),\n",
       " (1358, 609),\n",
       " (1358, 613),\n",
       " (1358, 616),\n",
       " (1358, 626),\n",
       " (1358, 645),\n",
       " (1358, 684),\n",
       " (1358, 686),\n",
       " (1358, 687),\n",
       " (1358, 689),\n",
       " (1358, 708),\n",
       " (1358, 744),\n",
       " (1358, 748),\n",
       " (1358, 754),\n",
       " (1358, 757),\n",
       " (1358, 758),\n",
       " (1358, 764),\n",
       " (1358, 796),\n",
       " (1358, 797),\n",
       " (1358, 819),\n",
       " (1358, 823),\n",
       " (1358, 831),\n",
       " (1358, 853),\n",
       " (1358, 873),\n",
       " (1358, 876),\n",
       " (1358, 882),\n",
       " (1358, 902),\n",
       " (1358, 919),\n",
       " (1358, 929),\n",
       " (1358, 951),\n",
       " (1358, 957),\n",
       " (1358, 978),\n",
       " (1358, 985),\n",
       " (1358, 999),\n",
       " (1358, 1038),\n",
       " (1358, 1040),\n",
       " (1358, 1041),\n",
       " (1358, 1070),\n",
       " (1358, 1103),\n",
       " (1358, 1123),\n",
       " (1358, 1145),\n",
       " (1358, 1149),\n",
       " (1358, 1154),\n",
       " (1358, 1169),\n",
       " (1358, 1189),\n",
       " (1358, 1205),\n",
       " (1358, 1209),\n",
       " (1358, 1229),\n",
       " (1358, 1238),\n",
       " (1358, 1243),\n",
       " (1358, 1281),\n",
       " (1358, 1284),\n",
       " (1358, 1287),\n",
       " (1358, 1292),\n",
       " (1358, 1305),\n",
       " (1358, 1339),\n",
       " (1358, 1384),\n",
       " (1358, 1389),\n",
       " (1358, 1444),\n",
       " (1358, 1471),\n",
       " (1358, 1480),\n",
       " (1358, 1483),\n",
       " (1358, 1492),\n",
       " (1358, 1499),\n",
       " (1358, 1516),\n",
       " (1358, 1546),\n",
       " (1358, 1550),\n",
       " (1358, 1562),\n",
       " (1358, 1565),\n",
       " (1358, 1568),\n",
       " (1358, 1599),\n",
       " (1358, 1604),\n",
       " (1358, 1620),\n",
       " (1358, 1646),\n",
       " (1358, 1708),\n",
       " (1358, 1709),\n",
       " (1358, 1710),\n",
       " (1358, 1711),\n",
       " (1358, 1712),\n",
       " (1358, 1713),\n",
       " (1358, 1714),\n",
       " (1358, 1715),\n",
       " (1358, 1716),\n",
       " (1358, 1717),\n",
       " (1358, 1718),\n",
       " (1358, 1719),\n",
       " (1358, 1720),\n",
       " (1358, 1721),\n",
       " (1358, 1722),\n",
       " (1358, 1723),\n",
       " (1358, 1724),\n",
       " (1358, 1725),\n",
       " (1358, 1726),\n",
       " (1358, 1727),\n",
       " (1358, 1728),\n",
       " (1358, 1729),\n",
       " (1358, 1730),\n",
       " (1358, 1731),\n",
       " (1358, 1732),\n",
       " (1358, 1733),\n",
       " (1358, 1734),\n",
       " (1358, 1735),\n",
       " (1358, 1736),\n",
       " (1358, 1737),\n",
       " (1358, 1738),\n",
       " (1358, 1739),\n",
       " (1358, 1740),\n",
       " (1358, 1741),\n",
       " (1358, 1742),\n",
       " (1358, 1743),\n",
       " (1358, 1744),\n",
       " (1358, 1745),\n",
       " (1358, 1746),\n",
       " (1358, 1747),\n",
       " (1358, 1748),\n",
       " (1358, 1749),\n",
       " (1358, 1750),\n",
       " (1358, 1751),\n",
       " (1358, 1752),\n",
       " (1358, 1753),\n",
       " (1358, 1754),\n",
       " (1358, 1755),\n",
       " (1358, 1756),\n",
       " (1358, 1757),\n",
       " (1358, 1758),\n",
       " (1358, 1759),\n",
       " (1358, 1760),\n",
       " (1358, 1762),\n",
       " (1358, 1763),\n",
       " (1358, 1764),\n",
       " (1358, 1765),\n",
       " (1358, 1766),\n",
       " (1358, 2597),\n",
       " (2162, 61),\n",
       " (2162, 1080),\n",
       " (2162, 2163),\n",
       " (2343, 2344),\n",
       " (31, 1594),\n",
       " (32, 279),\n",
       " (32, 518),\n",
       " (32, 1850),\n",
       " (32, 1973),\n",
       " (279, 242),\n",
       " (279, 270),\n",
       " (279, 304),\n",
       " (279, 502),\n",
       " (279, 666),\n",
       " (279, 838),\n",
       " (279, 1195),\n",
       " (279, 2165),\n",
       " (279, 2280),\n",
       " (279, 2344),\n",
       " (279, 2423),\n",
       " (518, 666),\n",
       " (518, 1373),\n",
       " (518, 2423),\n",
       " (1850, 415),\n",
       " (1850, 666),\n",
       " (1850, 1013),\n",
       " (1973, 252),\n",
       " (1973, 490),\n",
       " (1973, 504),\n",
       " (1973, 666),\n",
       " (1973, 779),\n",
       " (1973, 822),\n",
       " (1973, 1333),\n",
       " (1973, 1482),\n",
       " (1973, 1525),\n",
       " (1973, 1637),\n",
       " (1973, 1683),\n",
       " (1973, 1974),\n",
       " (1973, 1975),\n",
       " (1973, 1976),\n",
       " (33, 286),\n",
       " (33, 588),\n",
       " (33, 698),\n",
       " (33, 911),\n",
       " (33, 1051),\n",
       " (33, 2040),\n",
       " (33, 2119),\n",
       " (33, 2120),\n",
       " (33, 2121),\n",
       " (286, 442),\n",
       " (286, 698),\n",
       " (588, 442),\n",
       " (588, 698),\n",
       " (588, 2383),\n",
       " (698, 442),\n",
       " (698, 1015),\n",
       " (911, 893),\n",
       " (911, 1051),\n",
       " (911, 1534),\n",
       " (911, 2376),\n",
       " (1051, 442),\n",
       " (1051, 893),\n",
       " (1051, 2122),\n",
       " (1051, 2383),\n",
       " (2040, 2120),\n",
       " (2040, 2122),\n",
       " (2119, 218),\n",
       " (2120, 442),\n",
       " (2120, 665),\n",
       " (2120, 2637),\n",
       " (2121, 2001),\n",
       " (2121, 2122),\n",
       " (35, 895),\n",
       " (35, 1296),\n",
       " (35, 1913),\n",
       " (895, 440),\n",
       " (895, 1187),\n",
       " (895, 1296),\n",
       " (895, 1913),\n",
       " (1296, 440),\n",
       " (1296, 728),\n",
       " (1296, 1910),\n",
       " (1296, 1911),\n",
       " (1296, 1912),\n",
       " (1296, 1913),\n",
       " (1913, 440),\n",
       " (36, 1146),\n",
       " (36, 1505),\n",
       " (36, 1552),\n",
       " (36, 1640),\n",
       " (36, 1781),\n",
       " (36, 2094),\n",
       " (36, 2106),\n",
       " (36, 2107),\n",
       " (1146, 1505),\n",
       " (1146, 1506),\n",
       " (1146, 2254),\n",
       " (1505, 330),\n",
       " (1505, 530),\n",
       " (1505, 773),\n",
       " (1505, 1552),\n",
       " (1505, 1624),\n",
       " (1505, 1699),\n",
       " (1505, 1788),\n",
       " (1505, 1801),\n",
       " (1505, 2086),\n",
       " (1505, 2107),\n",
       " (1552, 1089),\n",
       " (1552, 1778),\n",
       " (1552, 2591),\n",
       " (1640, 306),\n",
       " (1640, 350),\n",
       " (1781, 306),\n",
       " (1781, 1623),\n",
       " (1781, 1998),\n",
       " (1781, 2107),\n",
       " (2094, 109),\n",
       " (2094, 441),\n",
       " (2094, 530),\n",
       " (2094, 2086),\n",
       " (2094, 2106),\n",
       " (2107, 399),\n",
       " (37, 60),\n",
       " (37, 1190),\n",
       " (37, 2427),\n",
       " (60, 55),\n",
       " (60, 1527),\n",
       " (1190, 1184),\n",
       " (1190, 1387),\n",
       " (2427, 349),\n",
       " (2427, 2428),\n",
       " (38, 429),\n",
       " (38, 862),\n",
       " (38, 863),\n",
       " (38, 1160),\n",
       " (429, 86),\n",
       " (429, 196),\n",
       " (429, 523),\n",
       " (429, 705),\n",
       " ...]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "G = nx.Graph()\n",
    "for i in range(data.edge_index.shape[1]):\n",
    "    source = data.edge_index[0, i].item()\n",
    "    target = data.edge_index[1, i].item()\n",
    "    G.add_edge(source, target)\n",
    "print(nx.components.is_connected(G))\n",
    "list(G.edges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 只考虑不同社团的情况\n",
    "def get_neg_node(data, node, partition, rows):\n",
    "\n",
    "    # Step 1: Convert Cora dataset to a NetworkX graph\n",
    "    G = nx.Graph()\n",
    "    for i in range(data.edge_index.shape[1]):\n",
    "        source = data.edge_index[0, i].item()\n",
    "        target = data.edge_index[1, i].item()\n",
    "        G.add_edge(source, target)\n",
    "    edges = list(G.edges)\n",
    "\n",
    "    # 选取最远距离的社团对应的节点作为负样本\n",
    "    community_graph, community_distances, max_dis_communities = construct_community_graph(edges, partition)\n",
    "\n",
    "    # print(community_distances)\n",
    "    # print()\n",
    "    # print(max_dis_communities)\n",
    "\n",
    "    # 创建一个新的字典，值是列表\n",
    "    flipped_partition = {}\n",
    "    for key, value in partition.items():\n",
    "        if value not in flipped_partition:\n",
    "            flipped_partition[value] = [key]\n",
    "        else:\n",
    "            flipped_partition[value].append(key)\n",
    "\n",
    "    # print(flipped_partition)\n",
    "    \n",
    "    neg_node = []\n",
    "    if len(neg_node) == rows.shape[0]:\n",
    "        neg_node = torch.stack(neg_node)\n",
    "        return neg_node\n",
    "    \n",
    "    else:\n",
    "        \n",
    "        neg_node = flipped_partition[max_dis_communities[partition[node.item()]]]\n",
    "        neg_node = torch.tensor(neg_node)\n",
    "        # print(neg_node)\n",
    "        if not node in neg_node:\n",
    "            # if neg_node.size(0) <= 1:\n",
    "            #     return neg_node\n",
    "            # else:\n",
    "            #     neg_node = torch.stack(neg_node)\n",
    "            return neg_node\n",
    "            \n",
    "        else:\n",
    "            neg_node = []\n",
    "            for target_node in torch.arange(data.x.size(0)):\n",
    "                if target_node != node:\n",
    "                    if torch.eq(rows, target_node).any():\n",
    "                        continue\n",
    "                    else:\n",
    "                        neg_node.append(target_node)\n",
    "                if len(neg_node) == rows.shape[0]:\n",
    "                    break\n",
    "            \n",
    "            neg_node = torch.stack(neg_node)\n",
    "            return neg_node\n",
    "\n",
    "# # 只考虑不同社团的情况\n",
    "# def get_neg_node(data, node, row):\n",
    "#     neg_node = []\n",
    "#     if len(neg_node) == row.size(0):\n",
    "#         neg_node = torch.stack(neg_node)\n",
    "#         return neg_node\n",
    "            \n",
    "#     else:\n",
    "#         for target_node in torch.arange(data.x.size(0)):\n",
    "#             if target_node != node:\n",
    "#                 if torch.eq(row, target_node).any():\n",
    "#                     continue\n",
    "#                 else:\n",
    "#                     neg_node.append(target_node)\n",
    "#             if len(neg_node) == row.size(0):\n",
    "#                 break\n",
    "        \n",
    "#         neg_node = torch.stack(neg_node)\n",
    "#         # print('*'*100)\n",
    "#         # print(neg_node, neg_node.shape)\n",
    "#         return neg_node\n",
    "\n",
    "# def get_neg_node(data, node, row):\n",
    "#     # 创建一个布尔掩码，标记哪些节点是正样本\n",
    "#     is_positive = torch.zeros(data.num_nodes, dtype=torch.bool)\n",
    "#     is_positive[row] = True\n",
    "\n",
    "#     # 通过异或操作找到负样本节点\n",
    "#     is_negative = ~is_positive\n",
    "\n",
    "#     # 去除自身节点\n",
    "#     is_negative[node] = False\n",
    "\n",
    "#     # 获取负样本节点的索引\n",
    "#     neg_node = torch.nonzero(is_negative, as_tuple=False).squeeze()\n",
    "#     print('*'*100)\n",
    "#     print(neg_node, neg_node.shape)\n",
    "#     return neg_node\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Data(x=[2, 1433], edge_index=[2, 2], y=[2], train_mask=[2], val_mask=[2], test_mask=[2]),\n",
       " Data(x=[2, 1433], edge_index=[2, 2], y=[2], train_mask=[2], val_mask=[2], test_mask=[2]))"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subgraphs[0], subgraphs[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim.lr_scheduler import StepLR\n",
    "import gc\n",
    "import warnings\n",
    "gc.collect()\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def objective(trial):\n",
    "    alpha = trial.suggest_float('alpha', 0.01, 0.5)\n",
    "    lr = trial.suggest_float('lr', 1e-5, 1e-2)\n",
    "\n",
    "    model = GCN(data.num_features, 512, dataset.num_classes)\n",
    "    best_val_acc = 0\n",
    "    num_epochs = 50\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.005, weight_decay=5e-4)\n",
    "    scheduler = StepLR(optimizer, step_size=30, gamma=0.5)\n",
    "    cross_entropy = torch.nn.CrossEntropyLoss()\n",
    "    for epoch in tqdm(range(num_epochs)):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        emb, out = model(data.x, data.edge_index)\n",
    "        loss1 = cross_entropy(out[data.train_mask], data.y[data.train_mask])\n",
    "        same_com_cos_sim, diff_com_cos_sim = [], []\n",
    "        \n",
    "        emb = emb - torch.mean(emb)\n",
    "        train_losses = []\n",
    "        for node in torch.arange(data.x.size(0))[data.train_mask]:\n",
    "            # for the same community\n",
    "            col = (community_masks[node, :] == 1).nonzero().view(-1)\n",
    "            \n",
    "            rows = (community_masks[:, col] == 1).nonzero()[:,0].view(-1)\n",
    "        \n",
    "            cos_sims = []\n",
    "            for row in rows:\n",
    "                if node != row:\n",
    "                    cos_sim = torch.cosine_similarity(emb[node,:], emb[row,:], dim=0)\n",
    "                    cos_sims.append(cos_sim)\n",
    "            cos_sims = torch.stack(cos_sims)\n",
    "            cos_sims = F.normalize(cos_sims, dim=0)\n",
    "            # print(cos_sims, cos_sims.shape)\n",
    "            cos_sims = torch.mean(cos_sims)\n",
    "            \n",
    "            same_com_cos_sim.append(cos_sims.unsqueeze(dim=0))\n",
    "\n",
    "            # for different communities\n",
    "            neg_node = get_neg_node(data, node, partition, rows)\n",
    "            neg_cos_sims = []\n",
    "            for neg in neg_node:\n",
    "                neg_cos_sim = F.cosine_similarity(emb[node,:], emb[neg,:], dim=0)\n",
    "                neg_cos_sims.append(neg_cos_sim)\n",
    "            neg_cos_sims = torch.stack(neg_cos_sims)\n",
    "            neg_cos_sims = F.normalize(neg_cos_sims, dim=0)\n",
    "            neg_cos_sims = torch.mean(neg_cos_sims)\n",
    "            \n",
    "            diff_com_cos_sim.append(neg_cos_sims.unsqueeze(dim=0))\n",
    "\n",
    "        \n",
    "        same_com_cos_sim = torch.stack(same_com_cos_sim)\n",
    "        same_com_cos_sim = same_com_cos_sim.squeeze(dim=1)\n",
    "        same_com_dis = 1 - same_com_cos_sim\n",
    "\n",
    "        diff_com_cos_sim = torch.stack(diff_com_cos_sim)\n",
    "        diff_com_cos_sim = diff_com_cos_sim.squeeze(dim=1)\n",
    "\n",
    "        diff_com_dis = 1 - diff_com_cos_sim\n",
    "\n",
    "        loss2 = diff_com_dis - same_com_dis\n",
    "        loss2 = torch.mean(loss2)\n",
    "        \n",
    "\n",
    "        loss = torch.mean(loss1) - alpha * loss2\n",
    "        train_losses.append(loss)\n",
    "        # print('=======loss:')\n",
    "        # print(loss, loss1, loss2)\n",
    "        # loss  =loss1\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "\n",
    "        # 模型验证\n",
    "        best_val_acc = 0\n",
    "        val_losses = []\n",
    "        with torch.no_grad():\n",
    "            model.eval()\n",
    "            emb, out = model(data.x, data.edge_index)\n",
    "            emb = emb - torch.mean(emb)\n",
    "            val_loss1 = cross_entropy(out[data.val_mask], data.y[data.val_mask])\n",
    "            same_com_cos_sim, diff_com_cos_sim = [], []\n",
    "            for node in torch.arange(data.x.size(0))[data.val_mask]:\n",
    "                # for nodes from the same community\n",
    "                col = (community_masks[node, :] == 1).nonzero().view(-1)\n",
    "                rows = (community_masks[:, col] == 1).nonzero()[:,0].view(-1)\n",
    "\n",
    "                cos_sims = []\n",
    "                for row in rows:\n",
    "                    if node != row:\n",
    "                        cos_sim = torch.cosine_similarity(emb[node,:], emb[row,:], dim=0)\n",
    "                        cos_sims.append(cos_sim)\n",
    "\n",
    "                cos_sims = torch.stack(cos_sims)\n",
    "                cos_sims = F.normalize(cos_sims, dim=0)\n",
    "                cos_sims = torch.mean(cos_sims)\n",
    "                same_com_cos_sim.append(cos_sims.unsqueeze(dim=0))\n",
    "\n",
    "                # for nodes from different communities\n",
    "                neg_node = get_neg_node(data, node, partition, rows)\n",
    "                neg_cos_sims = []\n",
    "                for neg in neg_node:\n",
    "                    neg_cos_sim = F.cosine_similarity(emb[node,:], emb[neg,:], dim=0)\n",
    "                    neg_cos_sims.append(neg_cos_sim)\n",
    "                neg_cos_sims = torch.stack(neg_cos_sims)\n",
    "                neg_cos_sims = F.normalize(neg_cos_sims, dim=0)\n",
    "                neg_cos_sims = torch.mean(neg_cos_sims)\n",
    "                \n",
    "                diff_com_cos_sim.append(neg_cos_sims.unsqueeze(dim=0))\n",
    "\n",
    "            same_com_cos_sim = torch.stack(same_com_cos_sim)\n",
    "            same_com_cos_sim = same_com_cos_sim.squeeze(dim=1)\n",
    "            same_com_dis = 1 - same_com_cos_sim\n",
    "\n",
    "            diff_com_cos_sim = torch.stack(diff_com_cos_sim)\n",
    "            diff_com_cos_sim = diff_com_cos_sim.squeeze(dim=1)\n",
    "            diff_com_dis = 1 - diff_com_cos_sim\n",
    "            \n",
    "\n",
    "            val_loss2 = diff_com_dis - same_com_dis\n",
    "            val_loss2 = torch.mean(val_loss2)\n",
    "            \n",
    "            val_loss = torch.mean(val_loss1) - alpha * val_loss2\n",
    "            val_losses.append(val_loss.item())\n",
    "            # print(val_loss, val_loss1, val_loss2)\n",
    "            \n",
    "            val_pred = out.argmax(dim=1)\n",
    "            val_acc = accuracy(val_pred[data.val_mask], data.y[data.val_mask])\n",
    "        # 更新学习率\n",
    "        scheduler.step()\n",
    "        \n",
    "        if val_acc >= best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            best_model = copy.deepcopy(model)\n",
    "            torch.save(best_model.state_dict(), 'models/best_1026.pth')\n",
    "\n",
    "        if epoch % 2 == 0:\n",
    "                print(f'Epoch: {epoch:03d}, Loss: {loss.item():.4f}, Val Loss: {val_loss.item():.4f}, Val ACC: {val_acc:.4f}')\n",
    "\n",
    "    return best_val_acc\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-10-27 01:12:31,366] A new study created in memory with name: no-name-f3f539eb-951e-4d3b-8e11-a0df12e3f311\n",
      "  1%|          | 1/100 [01:02<1:43:10, 62.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 000, Loss: 1.9436, Val Loss: 1.9742, Val ACC: 0.2180\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 3/100 [03:05<1:39:53, 61.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 002, Loss: 1.9632, Val Loss: 1.9635, Val ACC: 0.3600\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 5/100 [05:07<1:36:53, 61.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 004, Loss: 1.9344, Val Loss: 1.9513, Val ACC: 0.3300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 7/100 [07:10<1:35:16, 61.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 006, Loss: 1.8986, Val Loss: 1.9333, Val ACC: 0.3420\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 8/100 [09:12<1:45:53, 69.06s/it]\n",
      "[W 2023-10-27 01:21:43,897] Trial 0 failed with parameters: {'alpha': 0.4288404211756271, 'lr': 0.009267638203603984} because of the following error: KeyboardInterrupt().\n",
      "Traceback (most recent call last):\n",
      "  File \"/users/Min/miniconda/lib/python3.11/site-packages/optuna/study/_optimize.py\", line 200, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "                      ^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_1509293/3551570483.py\", line 105, in objective\n",
      "    neg_node = get_neg_node(data, node, partition, rows)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_1509293/452686648.py\", line 8, in get_neg_node\n",
      "    target = data.edge_index[1, i].item()\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "KeyboardInterrupt\n",
      "[W 2023-10-27 01:21:43,899] Trial 0 failed with value None.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/users/Min/MadGap/cora_ana_1026.ipynb 单元格 33\u001b[0m line \u001b[0;36m3\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22736d323230752d31307331303633332e776973632e636c6f75646c61622e7573222c2275736572223a224d696e227d/users/Min/MadGap/cora_ana_1026.ipynb#X44sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# create a study object to manage the optimization\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22736d323230752d31307331303633332e776973632e636c6f75646c61622e7573222c2275736572223a224d696e227d/users/Min/MadGap/cora_ana_1026.ipynb#X44sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m study \u001b[39m=\u001b[39m optuna\u001b[39m.\u001b[39mcreate_study(direction\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mmaximize\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22736d323230752d31307331303633332e776973632e636c6f75646c61622e7573222c2275736572223a224d696e227d/users/Min/MadGap/cora_ana_1026.ipynb#X44sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m study\u001b[39m.\u001b[39;49moptimize(objective, n_trials\u001b[39m=\u001b[39;49m\u001b[39m3\u001b[39;49m)\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22736d323230752d31307331303633332e776973632e636c6f75646c61622e7573222c2275736572223a224d696e227d/users/Min/MadGap/cora_ana_1026.ipynb#X44sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39m# get the best hyperparameters from the optimization\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22736d323230752d31307331303633332e776973632e636c6f75646c61622e7573222c2275736572223a224d696e227d/users/Min/MadGap/cora_ana_1026.ipynb#X44sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m best_alpha \u001b[39m=\u001b[39m study\u001b[39m.\u001b[39mbest_params[\u001b[39m'\u001b[39m\u001b[39malpha\u001b[39m\u001b[39m'\u001b[39m]\n",
      "File \u001b[0;32m~/miniconda/lib/python3.11/site-packages/optuna/study/study.py:451\u001b[0m, in \u001b[0;36mStudy.optimize\u001b[0;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m    348\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39moptimize\u001b[39m(\n\u001b[1;32m    349\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    350\u001b[0m     func: ObjectiveFuncType,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    357\u001b[0m     show_progress_bar: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m    358\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    359\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Optimize an objective function.\u001b[39;00m\n\u001b[1;32m    360\u001b[0m \n\u001b[1;32m    361\u001b[0m \u001b[39m    Optimization is done by choosing a suitable set of hyperparameter values from a given\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    449\u001b[0m \u001b[39m            If nested invocation of this method occurs.\u001b[39;00m\n\u001b[1;32m    450\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 451\u001b[0m     _optimize(\n\u001b[1;32m    452\u001b[0m         study\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m,\n\u001b[1;32m    453\u001b[0m         func\u001b[39m=\u001b[39;49mfunc,\n\u001b[1;32m    454\u001b[0m         n_trials\u001b[39m=\u001b[39;49mn_trials,\n\u001b[1;32m    455\u001b[0m         timeout\u001b[39m=\u001b[39;49mtimeout,\n\u001b[1;32m    456\u001b[0m         n_jobs\u001b[39m=\u001b[39;49mn_jobs,\n\u001b[1;32m    457\u001b[0m         catch\u001b[39m=\u001b[39;49m\u001b[39mtuple\u001b[39;49m(catch) \u001b[39mif\u001b[39;49;00m \u001b[39misinstance\u001b[39;49m(catch, Iterable) \u001b[39melse\u001b[39;49;00m (catch,),\n\u001b[1;32m    458\u001b[0m         callbacks\u001b[39m=\u001b[39;49mcallbacks,\n\u001b[1;32m    459\u001b[0m         gc_after_trial\u001b[39m=\u001b[39;49mgc_after_trial,\n\u001b[1;32m    460\u001b[0m         show_progress_bar\u001b[39m=\u001b[39;49mshow_progress_bar,\n\u001b[1;32m    461\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda/lib/python3.11/site-packages/optuna/study/_optimize.py:66\u001b[0m, in \u001b[0;36m_optimize\u001b[0;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     65\u001b[0m     \u001b[39mif\u001b[39;00m n_jobs \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m---> 66\u001b[0m         _optimize_sequential(\n\u001b[1;32m     67\u001b[0m             study,\n\u001b[1;32m     68\u001b[0m             func,\n\u001b[1;32m     69\u001b[0m             n_trials,\n\u001b[1;32m     70\u001b[0m             timeout,\n\u001b[1;32m     71\u001b[0m             catch,\n\u001b[1;32m     72\u001b[0m             callbacks,\n\u001b[1;32m     73\u001b[0m             gc_after_trial,\n\u001b[1;32m     74\u001b[0m             reseed_sampler_rng\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m     75\u001b[0m             time_start\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m     76\u001b[0m             progress_bar\u001b[39m=\u001b[39;49mprogress_bar,\n\u001b[1;32m     77\u001b[0m         )\n\u001b[1;32m     78\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     79\u001b[0m         \u001b[39mif\u001b[39;00m n_jobs \u001b[39m==\u001b[39m \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m:\n",
      "File \u001b[0;32m~/miniconda/lib/python3.11/site-packages/optuna/study/_optimize.py:163\u001b[0m, in \u001b[0;36m_optimize_sequential\u001b[0;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[1;32m    160\u001b[0m         \u001b[39mbreak\u001b[39;00m\n\u001b[1;32m    162\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 163\u001b[0m     frozen_trial \u001b[39m=\u001b[39m _run_trial(study, func, catch)\n\u001b[1;32m    164\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m    165\u001b[0m     \u001b[39m# The following line mitigates memory problems that can be occurred in some\u001b[39;00m\n\u001b[1;32m    166\u001b[0m     \u001b[39m# environments (e.g., services that use computing containers such as GitHub Actions).\u001b[39;00m\n\u001b[1;32m    167\u001b[0m     \u001b[39m# Please refer to the following PR for further details:\u001b[39;00m\n\u001b[1;32m    168\u001b[0m     \u001b[39m# https://github.com/optuna/optuna/pull/325.\u001b[39;00m\n\u001b[1;32m    169\u001b[0m     \u001b[39mif\u001b[39;00m gc_after_trial:\n",
      "File \u001b[0;32m~/miniconda/lib/python3.11/site-packages/optuna/study/_optimize.py:251\u001b[0m, in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    244\u001b[0m         \u001b[39massert\u001b[39;00m \u001b[39mFalse\u001b[39;00m, \u001b[39m\"\u001b[39m\u001b[39mShould not reach.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    246\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[1;32m    247\u001b[0m     frozen_trial\u001b[39m.\u001b[39mstate \u001b[39m==\u001b[39m TrialState\u001b[39m.\u001b[39mFAIL\n\u001b[1;32m    248\u001b[0m     \u001b[39mand\u001b[39;00m func_err \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    249\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(func_err, catch)\n\u001b[1;32m    250\u001b[0m ):\n\u001b[0;32m--> 251\u001b[0m     \u001b[39mraise\u001b[39;00m func_err\n\u001b[1;32m    252\u001b[0m \u001b[39mreturn\u001b[39;00m frozen_trial\n",
      "File \u001b[0;32m~/miniconda/lib/python3.11/site-packages/optuna/study/_optimize.py:200\u001b[0m, in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[39mwith\u001b[39;00m get_heartbeat_thread(trial\u001b[39m.\u001b[39m_trial_id, study\u001b[39m.\u001b[39m_storage):\n\u001b[1;32m    199\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 200\u001b[0m         value_or_values \u001b[39m=\u001b[39m func(trial)\n\u001b[1;32m    201\u001b[0m     \u001b[39mexcept\u001b[39;00m exceptions\u001b[39m.\u001b[39mTrialPruned \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    202\u001b[0m         \u001b[39m# TODO(mamu): Handle multi-objective cases.\u001b[39;00m\n\u001b[1;32m    203\u001b[0m         state \u001b[39m=\u001b[39m TrialState\u001b[39m.\u001b[39mPRUNED\n",
      "\u001b[1;32m/users/Min/MadGap/cora_ana_1026.ipynb 单元格 33\u001b[0m line \u001b[0;36m1\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22736d323230752d31307331303633332e776973632e636c6f75646c61622e7573222c2275736572223a224d696e227d/users/Min/MadGap/cora_ana_1026.ipynb#X44sdnNjb2RlLXJlbW90ZQ%3D%3D?line=101'>102</a>\u001b[0m same_com_cos_sim\u001b[39m.\u001b[39mappend(cos_sims\u001b[39m.\u001b[39munsqueeze(dim\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m))\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22736d323230752d31307331303633332e776973632e636c6f75646c61622e7573222c2275736572223a224d696e227d/users/Min/MadGap/cora_ana_1026.ipynb#X44sdnNjb2RlLXJlbW90ZQ%3D%3D?line=103'>104</a>\u001b[0m \u001b[39m# for nodes from different communities\u001b[39;00m\n\u001b[0;32m--> <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22736d323230752d31307331303633332e776973632e636c6f75646c61622e7573222c2275736572223a224d696e227d/users/Min/MadGap/cora_ana_1026.ipynb#X44sdnNjb2RlLXJlbW90ZQ%3D%3D?line=104'>105</a>\u001b[0m neg_node \u001b[39m=\u001b[39m get_neg_node(data, node, partition, rows)\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22736d323230752d31307331303633332e776973632e636c6f75646c61622e7573222c2275736572223a224d696e227d/users/Min/MadGap/cora_ana_1026.ipynb#X44sdnNjb2RlLXJlbW90ZQ%3D%3D?line=105'>106</a>\u001b[0m neg_cos_sims \u001b[39m=\u001b[39m []\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22736d323230752d31307331303633332e776973632e636c6f75646c61622e7573222c2275736572223a224d696e227d/users/Min/MadGap/cora_ana_1026.ipynb#X44sdnNjb2RlLXJlbW90ZQ%3D%3D?line=106'>107</a>\u001b[0m \u001b[39mfor\u001b[39;00m neg \u001b[39min\u001b[39;00m neg_node:\n",
      "\u001b[1;32m/users/Min/MadGap/cora_ana_1026.ipynb 单元格 33\u001b[0m line \u001b[0;36m8\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22736d323230752d31307331303633332e776973632e636c6f75646c61622e7573222c2275736572223a224d696e227d/users/Min/MadGap/cora_ana_1026.ipynb#X44sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(data\u001b[39m.\u001b[39medge_index\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m]):\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22736d323230752d31307331303633332e776973632e636c6f75646c61622e7573222c2275736572223a224d696e227d/users/Min/MadGap/cora_ana_1026.ipynb#X44sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'>7</a>\u001b[0m     source \u001b[39m=\u001b[39m data\u001b[39m.\u001b[39medge_index[\u001b[39m0\u001b[39m, i]\u001b[39m.\u001b[39mitem()\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22736d323230752d31307331303633332e776973632e636c6f75646c61622e7573222c2275736572223a224d696e227d/users/Min/MadGap/cora_ana_1026.ipynb#X44sdnNjb2RlLXJlbW90ZQ%3D%3D?line=7'>8</a>\u001b[0m     target \u001b[39m=\u001b[39m data\u001b[39m.\u001b[39;49medge_index[\u001b[39m1\u001b[39;49m, i]\u001b[39m.\u001b[39;49mitem()\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22736d323230752d31307331303633332e776973632e636c6f75646c61622e7573222c2275736572223a224d696e227d/users/Min/MadGap/cora_ana_1026.ipynb#X44sdnNjb2RlLXJlbW90ZQ%3D%3D?line=8'>9</a>\u001b[0m     G\u001b[39m.\u001b[39madd_edge(source, target)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22736d323230752d31307331303633332e776973632e636c6f75646c61622e7573222c2275736572223a224d696e227d/users/Min/MadGap/cora_ana_1026.ipynb#X44sdnNjb2RlLXJlbW90ZQ%3D%3D?line=9'>10</a>\u001b[0m edges \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(G\u001b[39m.\u001b[39medges)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# create a study object to manage the optimization\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=3)\n",
    "\n",
    "# get the best hyperparameters from the optimization\n",
    "best_alpha = study.best_params['alpha']\n",
    "best_lr = study.best_params['lr']\n",
    "print(f'best alphs: {best_alpha}')\n",
    "print(f'best lr: {best_lr}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_lr = 0.25\n",
    "best_alpha = 0.0025"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 1/10 [01:01<09:15, 61.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 000, LR: 0.250000, Loss: 1.9461, Val Loss: 4.1737, Val ACC: 0.1700\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 3/10 [03:05<07:11, 61.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 002, LR: 0.250000, Loss: 9.3893, Val Loss: 17.4244, Val ACC: 0.3540\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 5/10 [05:08<05:07, 61.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 004, LR: 0.250000, Loss: 10.8519, Val Loss: 5.4782, Val ACC: 0.2920\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 7/10 [07:12<03:06, 62.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 006, LR: 0.250000, Loss: 0.6577, Val Loss: 1.6184, Val ACC: 0.5120\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 9/10 [09:16<01:01, 61.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 008, LR: 0.250000, Loss: 0.8579, Val Loss: 1.0150, Val ACC: 0.7320\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [10:18<00:00, 61.84s/it]\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "lr = best_lr\n",
    "alpha = best_alpha\n",
    "model = GCN(data.num_features, 512, dataset.num_classes)\n",
    "optimizer = optim.Adam([{'params': model.parameters()}], lr=lr)\n",
    "scheduler = StepLR(optimizer, step_size=30, gamma=0.5)\n",
    "cross_entropy = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "num_epochs = 10\n",
    "for epoch in tqdm(range(num_epochs)):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    emb, out = model(data.x, data.edge_index)\n",
    "    loss1 = cross_entropy(out[data.train_mask], data.y[data.train_mask])\n",
    "    same_com_cos_sim, diff_com_cos_sim = [], []\n",
    "    \n",
    "    emb = emb - torch.mean(emb)\n",
    "    train_losses = []\n",
    "    for node in torch.arange(data.x.size(0))[data.train_mask]:\n",
    "        # for the same community\n",
    "        col = (community_masks[node, :] == 1).nonzero().view(-1)\n",
    "        \n",
    "        rows = (community_masks[:, col] == 1).nonzero()[:,0].view(-1)\n",
    "    \n",
    "        cos_sims = []\n",
    "        for row in rows:\n",
    "            if node != row:\n",
    "                cos_sim = torch.cosine_similarity(emb[node,:], emb[row,:], dim=0)\n",
    "                cos_sims.append(cos_sim)\n",
    "        cos_sims = torch.stack(cos_sims)\n",
    "        cos_sims = F.normalize(cos_sims, dim=0)\n",
    "        # print(cos_sims, cos_sims.shape)\n",
    "        cos_sims = torch.mean(cos_sims)\n",
    "        \n",
    "        same_com_cos_sim.append(cos_sims.unsqueeze(dim=0))\n",
    "\n",
    "        # for different communities\n",
    "        neg_node = get_neg_node(data, node, partition, rows)\n",
    "        neg_cos_sims = []\n",
    "        for neg in neg_node:\n",
    "            neg_cos_sim = F.cosine_similarity(emb[node,:], emb[neg,:], dim=0)\n",
    "            neg_cos_sims.append(neg_cos_sim)\n",
    "        neg_cos_sims = torch.stack(neg_cos_sims)\n",
    "        neg_cos_sims = F.normalize(neg_cos_sims, dim=0)\n",
    "        neg_cos_sims = torch.mean(neg_cos_sims)\n",
    "        \n",
    "        diff_com_cos_sim.append(neg_cos_sims.unsqueeze(dim=0))\n",
    "\n",
    "    \n",
    "    same_com_cos_sim = torch.stack(same_com_cos_sim)\n",
    "    same_com_cos_sim = same_com_cos_sim.squeeze(dim=1)\n",
    "    same_com_dis = 1 - same_com_cos_sim\n",
    "\n",
    "    diff_com_cos_sim = torch.stack(diff_com_cos_sim)\n",
    "    diff_com_cos_sim = diff_com_cos_sim.squeeze(dim=1)\n",
    "\n",
    "    diff_com_dis = 1 - diff_com_cos_sim\n",
    "\n",
    "    loss2 = diff_com_dis - same_com_dis\n",
    "    loss2 = torch.mean(loss2)\n",
    "    \n",
    "\n",
    "    loss = torch.mean(loss1) - alpha * loss2\n",
    "    train_losses.append(loss)\n",
    "    # print('=======loss:')\n",
    "    # print(loss, loss1, loss2)\n",
    "    # loss  =loss1\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "\n",
    "    # 模型验证\n",
    "    best_val_acc = 0\n",
    "    val_losses = []\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        emb, out = model(data.x, data.edge_index)\n",
    "        emb = emb - torch.mean(emb)\n",
    "        val_loss1 = cross_entropy(out[data.val_mask], data.y[data.val_mask])\n",
    "        same_com_cos_sim, diff_com_cos_sim = [], []\n",
    "        for node in torch.arange(data.x.size(0))[data.val_mask]:\n",
    "            # for nodes from the same community\n",
    "            col = (community_masks[node, :] == 1).nonzero().view(-1)\n",
    "            rows = (community_masks[:, col] == 1).nonzero()[:,0].view(-1)\n",
    "\n",
    "            cos_sims = []\n",
    "            for row in rows:\n",
    "                if node != row:\n",
    "                    cos_sim = torch.cosine_similarity(emb[node,:], emb[row,:], dim=0)\n",
    "                    cos_sims.append(cos_sim)\n",
    "\n",
    "            cos_sims = torch.stack(cos_sims)\n",
    "            cos_sims = F.normalize(cos_sims, dim=0)\n",
    "            cos_sims = torch.mean(cos_sims)\n",
    "            same_com_cos_sim.append(cos_sims.unsqueeze(dim=0))\n",
    "\n",
    "            # for nodes from different communities\n",
    "            neg_node = get_neg_node(data, node, partition, rows)\n",
    "            neg_cos_sims = []\n",
    "            for neg in neg_node:\n",
    "                neg_cos_sim = F.cosine_similarity(emb[node,:], emb[neg,:], dim=0)\n",
    "                neg_cos_sims.append(neg_cos_sim)\n",
    "            neg_cos_sims = torch.stack(neg_cos_sims)\n",
    "            neg_cos_sims = F.normalize(neg_cos_sims, dim=0)\n",
    "            neg_cos_sims = torch.mean(neg_cos_sims)\n",
    "            diff_com_cos_sim.append(neg_cos_sims.unsqueeze(dim=0))\n",
    "\n",
    "        same_com_cos_sim = torch.stack(same_com_cos_sim)\n",
    "        same_com_cos_sim = same_com_cos_sim.squeeze(dim=1)\n",
    "        same_com_dis = 1 - same_com_cos_sim\n",
    "\n",
    "        diff_com_cos_sim = torch.stack(diff_com_cos_sim)\n",
    "        diff_com_cos_sim = diff_com_cos_sim.squeeze(dim=1)\n",
    "        diff_com_dis = 1 - diff_com_cos_sim\n",
    "        \n",
    "\n",
    "        val_loss2 = diff_com_dis - same_com_dis\n",
    "        val_loss2 = torch.mean(val_loss2)\n",
    "        \n",
    "        val_loss = torch.mean(val_loss1) - alpha * val_loss2\n",
    "        val_losses.append(val_loss.item())\n",
    "        # print(val_loss, val_loss1, val_loss2)\n",
    "        \n",
    "        val_pred = out.argmax(dim=1)\n",
    "        val_acc = accuracy(val_pred[data.val_mask], data.y[data.val_mask])\n",
    "    # 更新学习率\n",
    "    scheduler.step()\n",
    "    \n",
    "    if val_acc >= best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        best_model = copy.deepcopy(model)\n",
    "        torch.save(best_model.state_dict(), 'models/best_1026.pth')\n",
    "\n",
    "    if epoch % 2 == 0:\n",
    "            print(f'Epoch: {epoch:03d}, LR: {lr:.6f}, Loss: {loss.item():.4f}, Val Loss: {val_loss.item():.4f}, Val ACC: {val_acc:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the training and validation loss\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(train_losses, label='Train Loss')\n",
    "plt.plot(val_losses, label='Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.savefig('figs/loss_1025.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.6950\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "model = best_model\n",
    "emb, out = model(data.x, data.edge_index)\n",
    "test_pred = out.argmax(dim=1)\n",
    "test_acc = accuracy(test_pred[data.test_mask], data.y[data.test_mask])\n",
    "print(f'Test Accuracy: {test_acc:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
