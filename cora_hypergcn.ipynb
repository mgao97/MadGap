{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from copy import deepcopy\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from dhg import Hypergraph\n",
    "from dhg.data import Cooking200\n",
    "from dhg.nn import HGNNConv\n",
    "from dhg.random import set_seed\n",
    "from typing import Union, Dict, List\n",
    "from dhg.metrics.classification import VertexClassificationEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HGNN(nn.Module):\n",
    "    def __init__(self, in_channels, hid_channels, num_classes, use_bn, drop_rate=0.5):\n",
    "        super(HGNN, self).__init__()\n",
    "        self.layers = nn.ModuleList()\n",
    "        self.layers.append(\n",
    "            HGNNConv(in_channels, hid_channels, use_bn=use_bn, drop_rate=drop_rate)\n",
    "        )\n",
    "        self.layers.append(\n",
    "            HGNNConv(hid_channels, num_classes, use_bn=use_bn, is_last=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, X, hg):\n",
    "        for layer in self.layers:\n",
    "            X = layer(X, hg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HypergraphVertexClassificationEvaluator(VertexClassificationEvaluator):\n",
    "    def __init__(\n",
    "        self, metric_configs: List[Union[str, Dict[str, dict]]], validate_index: int = 0\n",
    "    ):\n",
    "        super(HypergraphVertexClassificationEvaluator ,self).__init__(metric_configs, validate_index)\n",
    "\n",
    "    def validate(self, y_true: torch.LongTensor, y_pred: torch.Tensor):\n",
    "        return super().validate(y_true, y_pred)\n",
    "\n",
    "\n",
    "    def test(self, y_true: torch.LongTensor, y_pred: torch.Tensor):\n",
    "        return super().test(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(net, X, A, lbls, train_idx, optimizer, epoch):\n",
    "    net.train()\n",
    "\n",
    "    st = time.time()\n",
    "    optimizer.zero_grad()\n",
    "    outs = net(X, A)\n",
    "    outs, lbls = outs[train_idx], lbls[train_idx]\n",
    "    loss = F.cross_entropy(outs, lbls)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    print(f\"Epoch: {epoch}, Time: {time.time()-st:.5f}s, Loss: {loss.item():.5f}\")\n",
    "    return loss.item()\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def infer(net, X, A, lbls, idx, test=False):\n",
    "    net.eval()\n",
    "    outs = net(X, A)\n",
    "    outs, lbls = outs[idx], lbls[idx]\n",
    "    if not test:\n",
    "        res = evaluator.validate(lbls, outs)\n",
    "    else:\n",
    "        res = evaluator.test(lbls, outs)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Time: 0.65269s, Loss: 3.01102\n",
      "update best: 0.05000\n",
      "Epoch: 1, Time: 0.42421s, Loss: 2.27640\n",
      "Epoch: 2, Time: 0.29357s, Loss: 2.14381\n",
      "Epoch: 3, Time: 0.29305s, Loss: 2.04013\n",
      "Epoch: 4, Time: 0.29318s, Loss: 1.98351\n",
      "Epoch: 5, Time: 0.30398s, Loss: 1.92233\n",
      "Epoch: 6, Time: 0.44434s, Loss: 1.87511\n",
      "Epoch: 7, Time: 0.41907s, Loss: 1.82192\n",
      "Epoch: 8, Time: 0.41729s, Loss: 1.76315\n",
      "update best: 0.08500\n",
      "Epoch: 9, Time: 0.45520s, Loss: 1.74401\n",
      "update best: 0.11500\n",
      "Epoch: 10, Time: 0.53395s, Loss: 1.71227\n",
      "Epoch: 11, Time: 0.42974s, Loss: 1.66392\n",
      "Epoch: 12, Time: 0.43261s, Loss: 1.62948\n",
      "Epoch: 13, Time: 0.44239s, Loss: 1.60624\n",
      "Epoch: 14, Time: 0.44863s, Loss: 1.57577\n",
      "Epoch: 15, Time: 0.59132s, Loss: 1.55663\n",
      "Epoch: 16, Time: 0.43944s, Loss: 1.50966\n",
      "Epoch: 17, Time: 0.54647s, Loss: 1.48429\n",
      "Epoch: 18, Time: 0.42638s, Loss: 1.44926\n",
      "Epoch: 19, Time: 0.42827s, Loss: 1.43334\n",
      "Epoch: 20, Time: 0.42928s, Loss: 1.42709\n",
      "Epoch: 21, Time: 0.43701s, Loss: 1.38506\n",
      "Epoch: 22, Time: 0.42893s, Loss: 1.35650\n",
      "Epoch: 23, Time: 0.41562s, Loss: 1.34163\n",
      "Epoch: 24, Time: 0.42426s, Loss: 1.32817\n",
      "Epoch: 25, Time: 0.42591s, Loss: 1.31012\n",
      "Epoch: 26, Time: 0.43625s, Loss: 1.27508\n",
      "Epoch: 27, Time: 0.42755s, Loss: 1.28574\n",
      "Epoch: 28, Time: 0.41845s, Loss: 1.24853\n",
      "Epoch: 29, Time: 0.40089s, Loss: 1.23296\n",
      "Epoch: 30, Time: 0.30302s, Loss: 1.22015\n",
      "Epoch: 31, Time: 0.30318s, Loss: 1.20411\n",
      "Epoch: 32, Time: 0.31924s, Loss: 1.17950\n",
      "Epoch: 33, Time: 0.41622s, Loss: 1.14547\n",
      "Epoch: 34, Time: 0.51978s, Loss: 1.14295\n",
      "Epoch: 35, Time: 0.42014s, Loss: 1.12691\n",
      "Epoch: 36, Time: 0.43949s, Loss: 1.11033\n",
      "Epoch: 37, Time: 0.40661s, Loss: 1.09971\n",
      "Epoch: 38, Time: 0.43825s, Loss: 1.08869\n",
      "Epoch: 39, Time: 0.43814s, Loss: 1.08846\n",
      "Epoch: 40, Time: 0.42713s, Loss: 1.06756\n",
      "update best: 0.12000\n",
      "Epoch: 41, Time: 0.43023s, Loss: 1.05032\n",
      "update best: 0.12500\n",
      "Epoch: 42, Time: 0.42707s, Loss: 1.04540\n",
      "update best: 0.13500\n",
      "Epoch: 43, Time: 0.42589s, Loss: 1.03683\n",
      "update best: 0.14500\n",
      "Epoch: 44, Time: 0.43369s, Loss: 0.99847\n",
      "Epoch: 45, Time: 0.42785s, Loss: 1.00823\n",
      "Epoch: 46, Time: 0.42598s, Loss: 0.98708\n",
      "update best: 0.16500\n",
      "Epoch: 47, Time: 0.43224s, Loss: 0.96715\n",
      "update best: 0.19000\n",
      "Epoch: 48, Time: 0.42619s, Loss: 0.96849\n",
      "update best: 0.22000\n",
      "Epoch: 49, Time: 0.42998s, Loss: 0.94906\n",
      "update best: 0.25500\n",
      "Epoch: 50, Time: 0.43166s, Loss: 0.95228\n",
      "update best: 0.27500\n",
      "Epoch: 51, Time: 0.42097s, Loss: 0.92062\n",
      "update best: 0.28500\n",
      "Epoch: 52, Time: 0.43233s, Loss: 0.92337\n",
      "update best: 0.29500\n",
      "Epoch: 53, Time: 0.43005s, Loss: 0.91337\n",
      "update best: 0.31000\n",
      "Epoch: 54, Time: 0.42640s, Loss: 0.89447\n",
      "update best: 0.32000\n",
      "Epoch: 55, Time: 0.44137s, Loss: 0.90781\n",
      "Epoch: 56, Time: 0.45447s, Loss: 0.88069\n",
      "Epoch: 57, Time: 0.42930s, Loss: 0.86817\n",
      "update best: 0.33500\n",
      "Epoch: 58, Time: 0.43645s, Loss: 0.86714\n",
      "update best: 0.34500\n",
      "Epoch: 59, Time: 0.43706s, Loss: 0.85128\n",
      "update best: 0.38000\n",
      "Epoch: 60, Time: 0.43996s, Loss: 0.84091\n",
      "update best: 0.39000\n",
      "Epoch: 61, Time: 0.43983s, Loss: 0.82617\n",
      "update best: 0.39500\n",
      "Epoch: 62, Time: 0.43686s, Loss: 0.82119\n",
      "update best: 0.42000\n",
      "Epoch: 63, Time: 0.42807s, Loss: 0.82343\n",
      "update best: 0.42500\n",
      "Epoch: 64, Time: 0.44446s, Loss: 0.81777\n",
      "update best: 0.43500\n",
      "Epoch: 65, Time: 0.44046s, Loss: 0.80214\n",
      "update best: 0.44000\n",
      "Epoch: 66, Time: 0.44526s, Loss: 0.77337\n",
      "update best: 0.45000\n",
      "Epoch: 67, Time: 0.43133s, Loss: 0.79730\n",
      "Epoch: 68, Time: 0.43294s, Loss: 0.76660\n",
      "Epoch: 69, Time: 0.44380s, Loss: 0.75498\n",
      "Epoch: 70, Time: 0.44722s, Loss: 0.74192\n",
      "Epoch: 71, Time: 0.44686s, Loss: 0.73955\n",
      "Epoch: 72, Time: 0.44492s, Loss: 0.76384\n",
      "Epoch: 73, Time: 0.42620s, Loss: 0.73955\n",
      "Epoch: 74, Time: 0.37971s, Loss: 0.73846\n",
      "Epoch: 75, Time: 0.44987s, Loss: 0.71575\n",
      "update best: 0.45500\n",
      "Epoch: 76, Time: 0.43372s, Loss: 0.72059\n",
      "Epoch: 77, Time: 0.44822s, Loss: 0.72133\n",
      "Epoch: 78, Time: 0.43445s, Loss: 0.70154\n",
      "Epoch: 79, Time: 0.44008s, Loss: 0.70230\n",
      "Epoch: 80, Time: 0.43983s, Loss: 0.68929\n",
      "Epoch: 81, Time: 0.41023s, Loss: 0.68826\n",
      "Epoch: 82, Time: 0.32473s, Loss: 0.68448\n",
      "Epoch: 83, Time: 0.34512s, Loss: 0.67582\n",
      "Epoch: 84, Time: 0.30477s, Loss: 0.67669\n",
      "update best: 0.46000\n",
      "Epoch: 85, Time: 0.30272s, Loss: 0.64981\n",
      "Epoch: 86, Time: 0.41633s, Loss: 0.64955\n",
      "Epoch: 87, Time: 0.42186s, Loss: 0.65548\n",
      "Epoch: 88, Time: 0.44316s, Loss: 0.62804\n",
      "Epoch: 89, Time: 0.42837s, Loss: 0.64509\n",
      "Epoch: 90, Time: 0.43334s, Loss: 0.62642\n",
      "Epoch: 91, Time: 0.42990s, Loss: 0.64445\n",
      "Epoch: 92, Time: 0.34635s, Loss: 0.62239\n",
      "Epoch: 93, Time: 0.30657s, Loss: 0.62881\n",
      "update best: 0.47000\n",
      "Epoch: 94, Time: 0.30812s, Loss: 0.61431\n",
      "update best: 0.48500\n",
      "Epoch: 95, Time: 0.30636s, Loss: 0.59744\n",
      "Epoch: 96, Time: 0.30675s, Loss: 0.60726\n",
      "Epoch: 97, Time: 0.30135s, Loss: 0.59486\n",
      "Epoch: 98, Time: 0.30477s, Loss: 0.60215\n",
      "Epoch: 99, Time: 0.30201s, Loss: 0.58006\n",
      "Epoch: 100, Time: 0.31133s, Loss: 0.56673\n",
      "Epoch: 101, Time: 0.30371s, Loss: 0.57215\n",
      "Epoch: 102, Time: 0.30208s, Loss: 0.57066\n",
      "Epoch: 103, Time: 0.42155s, Loss: 0.55599\n",
      "Epoch: 104, Time: 0.42364s, Loss: 0.55775\n",
      "Epoch: 105, Time: 0.41169s, Loss: 0.56257\n",
      "Epoch: 106, Time: 0.42033s, Loss: 0.52966\n",
      "Epoch: 107, Time: 0.43942s, Loss: 0.55732\n",
      "Epoch: 108, Time: 0.42331s, Loss: 0.53909\n",
      "Epoch: 109, Time: 0.43560s, Loss: 0.53125\n",
      "Epoch: 110, Time: 0.43108s, Loss: 0.54080\n",
      "Epoch: 111, Time: 0.42791s, Loss: 0.53153\n",
      "Epoch: 112, Time: 0.42850s, Loss: 0.52779\n",
      "Epoch: 113, Time: 0.42779s, Loss: 0.53931\n",
      "Epoch: 114, Time: 0.42497s, Loss: 0.52432\n",
      "Epoch: 115, Time: 0.42668s, Loss: 0.51756\n",
      "Epoch: 116, Time: 0.57081s, Loss: 0.51118\n",
      "Epoch: 117, Time: 0.30247s, Loss: 0.52879\n",
      "Epoch: 118, Time: 0.30192s, Loss: 0.50303\n",
      "Epoch: 119, Time: 0.30047s, Loss: 0.48028\n",
      "Epoch: 120, Time: 0.30269s, Loss: 0.50266\n",
      "Epoch: 121, Time: 0.30559s, Loss: 0.50028\n",
      "Epoch: 122, Time: 0.31413s, Loss: 0.48138\n",
      "Epoch: 123, Time: 0.30807s, Loss: 0.46776\n",
      "Epoch: 124, Time: 0.30360s, Loss: 0.48391\n",
      "Epoch: 125, Time: 0.43772s, Loss: 0.46384\n",
      "Epoch: 126, Time: 0.43205s, Loss: 0.47264\n",
      "Epoch: 127, Time: 0.43328s, Loss: 0.45249\n",
      "Epoch: 128, Time: 0.41428s, Loss: 0.45197\n",
      "Epoch: 129, Time: 0.42539s, Loss: 0.45834\n",
      "Epoch: 130, Time: 0.41974s, Loss: 0.45927\n",
      "Epoch: 131, Time: 0.44443s, Loss: 0.45552\n",
      "Epoch: 132, Time: 0.56649s, Loss: 0.44665\n",
      "Epoch: 133, Time: 0.41485s, Loss: 0.44511\n",
      "Epoch: 134, Time: 0.42653s, Loss: 0.43786\n",
      "Epoch: 135, Time: 0.42559s, Loss: 0.43823\n",
      "Epoch: 136, Time: 0.42673s, Loss: 0.44219\n",
      "Epoch: 137, Time: 0.44578s, Loss: 0.44364\n",
      "Epoch: 138, Time: 0.53726s, Loss: 0.42991\n",
      "Epoch: 139, Time: 0.43790s, Loss: 0.44061\n",
      "Epoch: 140, Time: 0.42170s, Loss: 0.43066\n",
      "Epoch: 141, Time: 0.44081s, Loss: 0.42100\n",
      "Epoch: 142, Time: 0.42775s, Loss: 0.41477\n",
      "Epoch: 143, Time: 0.42517s, Loss: 0.39532\n",
      "Epoch: 144, Time: 0.43378s, Loss: 0.41445\n",
      "Epoch: 145, Time: 0.44337s, Loss: 0.41587\n",
      "Epoch: 146, Time: 0.44517s, Loss: 0.44229\n",
      "Epoch: 147, Time: 0.54877s, Loss: 0.41695\n",
      "Epoch: 148, Time: 0.43880s, Loss: 0.39495\n",
      "Epoch: 149, Time: 0.42604s, Loss: 0.40900\n",
      "Epoch: 150, Time: 0.43019s, Loss: 0.40134\n",
      "Epoch: 151, Time: 0.43256s, Loss: 0.39409\n",
      "Epoch: 152, Time: 0.43345s, Loss: 0.41199\n",
      "Epoch: 153, Time: 0.42254s, Loss: 0.39515\n",
      "Epoch: 154, Time: 0.44005s, Loss: 0.37992\n",
      "Epoch: 155, Time: 0.42465s, Loss: 0.39470\n",
      "Epoch: 156, Time: 0.42245s, Loss: 0.38709\n",
      "Epoch: 157, Time: 0.43017s, Loss: 0.38332\n",
      "Epoch: 158, Time: 0.42657s, Loss: 0.37181\n",
      "Epoch: 159, Time: 0.42686s, Loss: 0.37749\n",
      "Epoch: 160, Time: 0.43265s, Loss: 0.37443\n",
      "Epoch: 161, Time: 0.42376s, Loss: 0.38549\n",
      "Epoch: 162, Time: 0.42877s, Loss: 0.37970\n",
      "Epoch: 163, Time: 0.43041s, Loss: 0.37119\n",
      "Epoch: 164, Time: 0.43042s, Loss: 0.36833\n",
      "Epoch: 165, Time: 0.45229s, Loss: 0.36183\n",
      "Epoch: 166, Time: 0.42551s, Loss: 0.38156\n",
      "Epoch: 167, Time: 0.44735s, Loss: 0.35413\n",
      "Epoch: 168, Time: 0.43710s, Loss: 0.35811\n",
      "Epoch: 169, Time: 0.44774s, Loss: 0.36324\n",
      "Epoch: 170, Time: 0.43848s, Loss: 0.34722\n",
      "Epoch: 171, Time: 0.44015s, Loss: 0.34106\n",
      "Epoch: 172, Time: 0.42618s, Loss: 0.36552\n",
      "Epoch: 173, Time: 0.43049s, Loss: 0.34302\n",
      "Epoch: 174, Time: 0.52955s, Loss: 0.33133\n",
      "Epoch: 175, Time: 0.42293s, Loss: 0.33642\n",
      "Epoch: 176, Time: 0.42751s, Loss: 0.32662\n",
      "Epoch: 177, Time: 0.43541s, Loss: 0.32933\n",
      "Epoch: 178, Time: 0.41922s, Loss: 0.32652\n",
      "Epoch: 179, Time: 0.46237s, Loss: 0.32236\n",
      "Epoch: 180, Time: 0.44257s, Loss: 0.33231\n",
      "Epoch: 181, Time: 0.44375s, Loss: 0.32347\n",
      "Epoch: 182, Time: 0.43438s, Loss: 0.33964\n",
      "Epoch: 183, Time: 0.42624s, Loss: 0.31180\n",
      "Epoch: 184, Time: 0.43856s, Loss: 0.30833\n",
      "Epoch: 185, Time: 0.43278s, Loss: 0.33044\n",
      "Epoch: 186, Time: 0.41978s, Loss: 0.33384\n",
      "Epoch: 187, Time: 0.43090s, Loss: 0.32158\n",
      "Epoch: 188, Time: 0.41923s, Loss: 0.31622\n",
      "Epoch: 189, Time: 0.42754s, Loss: 0.31103\n",
      "Epoch: 190, Time: 0.42819s, Loss: 0.31311\n",
      "Epoch: 191, Time: 0.41964s, Loss: 0.30874\n",
      "Epoch: 192, Time: 0.44455s, Loss: 0.31804\n",
      "Epoch: 193, Time: 0.45689s, Loss: 0.31282\n",
      "Epoch: 194, Time: 0.43300s, Loss: 0.30200\n",
      "Epoch: 195, Time: 0.43721s, Loss: 0.30069\n",
      "Epoch: 196, Time: 0.43551s, Loss: 0.28517\n",
      "Epoch: 197, Time: 0.46177s, Loss: 0.28884\n",
      "Epoch: 198, Time: 0.43140s, Loss: 0.30367\n",
      "Epoch: 199, Time: 0.42735s, Loss: 0.28769\n",
      "\n",
      "train finished!\n",
      "best val: 0.48500\n",
      "test...\n",
      "final result: epoch: 94\n",
      "{'accuracy': 0.5072112083435059, 'f1_score': 0.38390225669139444, 'f1_score -> average@micro': 0.5072111952020563}\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    set_seed(42)\n",
    "    device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "    evaluator = HypergraphVertexClassificationEvaluator([\"accuracy\", \"f1_score\", {\"f1_score\": {\"average\": \"micro\"}}])\n",
    "    data = Cooking200()\n",
    "\n",
    "    X, lbl = torch.eye(data[\"num_vertices\"]), data[\"labels\"]\n",
    "    G = Hypergraph(data[\"num_vertices\"], data[\"edge_list\"])\n",
    "    train_mask = data[\"train_mask\"]\n",
    "    val_mask = data[\"val_mask\"]\n",
    "    test_mask = data[\"test_mask\"]\n",
    "\n",
    "    net = HGNN(X.shape[1], 32, data[\"num_classes\"], use_bn=True)\n",
    "    optimizer = optim.Adam(net.parameters(), lr=0.01, weight_decay=5e-4)\n",
    "\n",
    "    X, lbl = X.to(device), lbl.to(device)\n",
    "    G = G.to(device)\n",
    "    net = net.to(device)\n",
    "\n",
    "    best_state = None\n",
    "    best_epoch, best_val = 0, 0\n",
    "    for epoch in range(200):\n",
    "        # train\n",
    "        train(net, X, G, lbl, train_mask, optimizer, epoch)\n",
    "        # validation\n",
    "        if epoch % 1 == 0:\n",
    "            with torch.no_grad():\n",
    "                val_res = infer(net, X, G, lbl, val_mask)\n",
    "            if val_res > best_val:\n",
    "                print(f\"update best: {val_res:.5f}\")\n",
    "                best_epoch = epoch\n",
    "                best_val = val_res\n",
    "                best_state = deepcopy(net.state_dict())\n",
    "    print(\"\\ntrain finished!\")\n",
    "    print(f\"best val: {best_val:.5f}\")\n",
    "    # test\n",
    "    print(\"test...\")\n",
    "    net.load_state_dict(best_state)\n",
    "    res = infer(net, X, G, lbl, test_mask, test=True)\n",
    "    print(f\"final result: epoch: {best_epoch}\")\n",
    "    print(res)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
